{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4119,
     "status": "ok",
     "timestamp": 1756957410691,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "gUtmxvpizzPl"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 1,
     "status": "ok",
     "timestamp": 1756957410694,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "c8MbgYdm1PzS"
   },
   "outputs": [],
   "source": [
    "def load_expert_results(expert_name):\n",
    "  result_all_runs = []\n",
    "  for run_number in range(1,6):\n",
    "    file_name = f\"Mixture of Experts/Results/{expert_name}_Run{run_number}.json\"\n",
    "    with open(file_name, 'r') as file:\n",
    "      results = json.load(file)\n",
    "    print(f\"Results loaded from {file_name}\")\n",
    "    # result_all_runs[f\"Run{run_number}\"] = results\n",
    "    result_all_runs.append(results)\n",
    "  return result_all_runs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ybl__cKR0Q7u"
   },
   "source": [
    "# Without caution\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 12478,
     "status": "ok",
     "timestamp": 1756957423662,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "v7cAvFzyMbLk",
    "outputId": "f626be73-7662-470b-8137-0afe75862f78"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random # Imported for the dummy function\n",
    "\n",
    "# Define class mappings\n",
    "class_mapping = {\"normal\": 0, \"abnormal\": 1}\n",
    "reverse_class_mapping = {v: k for k, v in class_mapping.items()}\n",
    "\n",
    "# # This is a placeholder. Replace with your actual data loading logic.\n",
    "# def load_expert_results(expert_name):\n",
    "#     \"\"\"Generates dummy prediction data for 5 runs.\"\"\"\n",
    "#     runs = []\n",
    "#     predictions = [\"normal\", \"abnormal\"]\n",
    "#     for _ in range(5):\n",
    "#         run_data = {\n",
    "#             \"image1.jpg\": {\"expectation\": \"normal\", \"prediction\": random.choice(predictions)},\n",
    "#             \"image2.jpg\": {\"expectation\": \"abnormal\", \"prediction\": random.choice(predictions)},\n",
    "#             \"image3.jpg\": {\"expectation\": \"normal\", \"prediction\": random.choice(predictions)},\n",
    "#             \"image4.jpg\": {\"expectation\": \"abnormal\", \"prediction\": random.choice(predictions)},\n",
    "#         }\n",
    "#         runs.append(run_data)\n",
    "#     return runs\n",
    "\n",
    "# Your expert list and data loading\n",
    "expert_list = ['resnet50_pretrained_cosine', 'resnet50_pretrained_one', 'resnet50_pretrained_euclidean', \"resnet50_pretrained_inf\",\"resnet50_pretrained_-inf\"]\n",
    "\n",
    "all_expert_5_runs_results = {\n",
    "    expert: load_expert_results(expert) for expert in expert_list\n",
    "}\n",
    "\n",
    "# Function to extract the \"output\" value from the prediction string\n",
    "def extract_output(prediction):\n",
    "    if isinstance(prediction, str):\n",
    "        return prediction.strip().lower()\n",
    "    elif isinstance(prediction, dict):\n",
    "        return prediction.get(\"output\", \"\").strip().lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "def get_run_wise_majority_vote(all_expert_results):\n",
    "    \"\"\"\n",
    "    Calculates detailed predictions for each run, segregated.\n",
    "\n",
    "    Args:\n",
    "        all_expert_results: A dictionary containing the results from all experts.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary where each key is a run number (e.g., \"run_1\") and the\n",
    "        value is another dictionary containing the run's image predictions.\n",
    "    \"\"\"\n",
    "    if not all_expert_results:\n",
    "        return {}\n",
    "\n",
    "    run_wise_final_predictions = {}\n",
    "    sample_expert = next(iter(all_expert_results.values()))\n",
    "    num_runs = len(sample_expert)\n",
    "    all_images = list(sample_expert[0].keys())\n",
    "\n",
    "    # Iterate through each run\n",
    "    for run_idx in range(num_runs):\n",
    "        predictions_for_this_run = {}\n",
    "        # Iterate through each image for the current run\n",
    "        for img in tqdm(all_images, desc=f\"Processing Run {run_idx + 1}\"):\n",
    "            image_results_for_run = {}\n",
    "            run_votes = []\n",
    "\n",
    "            # Collect predictions from all experts for this specific run and image\n",
    "            for expert_name, expert_runs in all_expert_results.items():\n",
    "                # Get the data for the current run\n",
    "                run_data = expert_runs[run_idx]\n",
    "                if img in run_data:\n",
    "                    prediction_data = run_data[img]\n",
    "                    pred_class = extract_output(prediction_data[\"prediction\"])\n",
    "\n",
    "                    # Add the actual class to the results (only needs to be done once per image)\n",
    "                    if \"actual_class\" not in image_results_for_run:\n",
    "                        actual_class = prediction_data[\"expectation\"].lower()\n",
    "                        image_results_for_run[\"actual_class\"] = actual_class\n",
    "\n",
    "                    if pred_class in class_mapping:\n",
    "                        # Store the individual expert's prediction for this run\n",
    "                        image_results_for_run[expert_name] = pred_class\n",
    "                        # Add the vote for the final majority calculation\n",
    "                        run_votes.append(class_mapping[pred_class])\n",
    "\n",
    "            # Determine the majority vote for this run among all experts\n",
    "            if run_votes:\n",
    "                final_vote_counts = Counter(run_votes)\n",
    "                final_majority_label = final_vote_counts.most_common(1)[0][0]\n",
    "                image_results_for_run[\"majority_voting_label\"] = reverse_class_mapping[final_majority_label]\n",
    "\n",
    "            predictions_for_this_run[img] = image_results_for_run\n",
    "\n",
    "        run_wise_final_predictions[f\"run_{run_idx + 1}\"] = predictions_for_this_run\n",
    "\n",
    "    return run_wise_final_predictions\n",
    "\n",
    "# Get the final, run-wise segregated predictions\n",
    "final_predictions = get_run_wise_majority_vote(all_expert_5_runs_results)\n",
    "\n",
    "# # Print the final predictions in a readable, segregated format\n",
    "# for run_name, run_predictions in final_predictions.items():\n",
    "#     print(f\"================== RESULTS FOR {run_name.upper()} ==================\")\n",
    "#     for img, predictions in run_predictions.items():\n",
    "#         print(f\"--- Predictions for: {img} ---\")\n",
    "#         print(json.dumps(predictions, indent=4))\n",
    "#     print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 15,
     "status": "ok",
     "timestamp": 1756957430452,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "O39RqEaNRkfA",
    "outputId": "443598fb-80a5-46a7-f98c-d24613fdd666"
   },
   "outputs": [],
   "source": [
    "final_predictions.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 25,
     "status": "ok",
     "timestamp": 1756957435975,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "Y2p8N3tpQPWk",
    "outputId": "1d7dcdab-888c-47a5-ae13-c544cc9b057a"
   },
   "outputs": [],
   "source": [
    "for run_name, run_predictions in final_predictions.items():\n",
    "    print(f\"================== RESULTS FOR {run_name.upper()} ==================\")\n",
    "    for img, predictions in run_predictions.items():\n",
    "        print(f\"--- Predictions for: {img} ---\")\n",
    "        print(json.dumps(predictions, indent=4))\n",
    "        break\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1587,
     "status": "ok",
     "timestamp": 1756957441236,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "Gdr0oguF1E28",
    "outputId": "b8ad1bbd-d0e6-4307-8313-97958e4ef384"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "from collections import Counter\n",
    "import random\n",
    "\n",
    "\n",
    "# --- NEW Function to Calculate Metrics and Plot ---\n",
    "\n",
    "def calculate_and_plot_metrics(final_predictions):\n",
    "    \"\"\"\n",
    "    Calculates metrics and plots confusion matrices for majority votes from final_predictions.\n",
    "\n",
    "    Args:\n",
    "        final_predictions: A dictionary structured by run, then by image, containing\n",
    "                           predictions and the actual class.\n",
    "    \"\"\"\n",
    "    num_runs = len(final_predictions)\n",
    "    if num_runs == 0:\n",
    "        print(\"No data to process.\")\n",
    "        return\n",
    "\n",
    "    # Prepare a figure for subplots (1 row, one plot per run)\n",
    "    fig, axes = plt.subplots(1, num_runs, figsize=(5 * num_runs, 5))\n",
    "    # If there's only one run, axes is not a list, so we make it one\n",
    "    if num_runs == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    all_metrics = {}\n",
    "\n",
    "    # Process results for each run\n",
    "    for idx, (run_name, run_predictions) in enumerate(final_predictions.items()):\n",
    "        actual = []\n",
    "        prediction = []\n",
    "\n",
    "        # Process each image in the run to get actual vs majority vote\n",
    "        for img, data in run_predictions.items():\n",
    "            if \"actual_class\" in data and \"majority_voting_label\" in data:\n",
    "                actual.append(class_mapping[data[\"actual_class\"]])\n",
    "                prediction.append(class_mapping[data[\"majority_voting_label\"]])\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(actual, prediction) * 100\n",
    "        precision = precision_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "        recall = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "        f1 = f1_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "        specificity = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"normal\"], zero_division=0) * 100\n",
    "\n",
    "        all_metrics[run_name] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"specificity\": specificity,\n",
    "        }\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        matrix = confusion_matrix(actual, prediction, labels=[0, 1])\n",
    "\n",
    "        # Plot confusion matrix in a subplot\n",
    "        ax = axes[idx]\n",
    "        cm_display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=list(class_mapping.keys()))\n",
    "        cm_display.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title(\n",
    "            f\"{run_name.replace('_', ' ').title()}\\n\"\n",
    "            f\"Acc: {accuracy:.2f}%, Prec: {precision:.2f}%\\n\"\n",
    "            f\"F1: {f1:.2f}%, Rec: {recall:.2f}%, Spec: {specificity:.2f}%\"\n",
    "        )\n",
    "\n",
    "    # Adjust layout and show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # Print metrics\n",
    "    print(\"\\n--- Summary of Metrics ---\")\n",
    "    for run_name, metric_values in all_metrics.items():\n",
    "        print(f\"\\nMetrics for {run_name}:\")\n",
    "        print(f\"  Accuracy: {metric_values['accuracy']:.2f}%\")\n",
    "        print(f\"  Precision: {metric_values['precision']:.2f}%\")\n",
    "        print(f\"  Recall (Sensitivity): {metric_values['recall']:.2f}%\")\n",
    "        print(f\"  F1 Score: {metric_values['f1']:.2f}%\")\n",
    "        print(f\"  Specificity: {metric_values['specificity']:.2f}%\")\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# 1. Generate the data using the function from the previous step\n",
    "expert_list = ['resnet50_pretrained_cosine', 'resnet50_pretrained_one', 'resnet50_pretrained_euclidean', \"resnet50_pretrained_inf\",\"resnet50_pretrained_-inf\"]\n",
    "all_expert_5_runs_results = {expert: load_expert_results(expert) for expert in expert_list}\n",
    "# final_predictions_data = get_run_wise_majority_vote(all_expert_5_runs_results)\n",
    "\n",
    "# 2. Call the new function to analyze and plot the results\n",
    "calculate_and_plot_metrics(final_predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 728,
     "status": "ok",
     "timestamp": 1756957817361,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "bzHQcseJz252",
    "outputId": "09ef1f54-eb4c-4127-b1ad-0aed6cbe84ac"
   },
   "outputs": [],
   "source": [
    "def calculate_plot_and_create_dataframe(final_predictions):\n",
    "    \"\"\"\n",
    "    Calculates metrics, plots confusion matrices, and returns a summary DataFrame.\n",
    "\n",
    "    Args:\n",
    "        final_predictions: A dictionary structured by run, then by image.\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame with metrics, including summary statistics, across all runs.\n",
    "    \"\"\"\n",
    "    num_runs = len(final_predictions)\n",
    "    if num_runs == 0:\n",
    "        print(\"No data to process.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    fig, axes = plt.subplots(1, num_runs, figsize=(5 * num_runs, 5))\n",
    "    if num_runs == 1:\n",
    "        axes = [axes]\n",
    "\n",
    "    all_metrics = {}\n",
    "\n",
    "    for idx, (run_name, run_predictions) in enumerate(final_predictions.items()):\n",
    "        actual, prediction = [], []\n",
    "        for img, data in run_predictions.items():\n",
    "            if \"actual_class\" in data and \"majority_voting_label\" in data:\n",
    "                actual.append(class_mapping[data[\"actual_class\"]])\n",
    "                prediction.append(class_mapping[data[\"majority_voting_label\"]])\n",
    "\n",
    "        accuracy = accuracy_score(actual, prediction) * 100\n",
    "        precision = precision_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "        recall = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "        f1 = f1_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "        specificity = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"normal\"], zero_division=0) * 100\n",
    "\n",
    "        all_metrics[run_name] = {\"accuracy\": accuracy, \"precision\": precision, \"recall\": recall, \"f1\": f1, \"specificity\": specificity}\n",
    "\n",
    "        matrix = confusion_matrix(actual, prediction, labels=[0, 1])\n",
    "        ax = axes[idx]\n",
    "        cm_display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=list(class_mapping.keys()))\n",
    "        cm_display.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title(f\"{run_name.replace('_', ' ').title()}\\nAcc: {accuracy:.2f}%\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "    # --- DataFrame Creation and Enhancement ---\n",
    "\n",
    "    df_data = {}\n",
    "    for run_name, metrics in all_metrics.items():\n",
    "        run_num = run_name.split('_')[-1]\n",
    "        for metric_name, value in metrics.items():\n",
    "            df_data[f\"{metric_name}_run{run_num}\"] = value\n",
    "\n",
    "    metrics_df = pd.DataFrame([df_data], index=['Majority Vote'])\n",
    "\n",
    "    metric_order = ['accuracy', 'precision', 'recall', 'specificity', 'f1']\n",
    "    final_column_order = []\n",
    "\n",
    "    for metric in metric_order:\n",
    "        run_cols = [f\"{metric}_run{i+1}\" for i in range(num_runs)]\n",
    "        final_column_order.extend(run_cols)\n",
    "\n",
    "        # Calculate summary stats for the current metric\n",
    "        avg = metrics_df[run_cols].iloc[0].mean()\n",
    "        var = metrics_df[run_cols].iloc[0].var()\n",
    "        std = metrics_df[run_cols].iloc[0].std()\n",
    "\n",
    "        # Add new stat columns to the DataFrame\n",
    "        metrics_df[f'{metric}_Avg'] = avg\n",
    "        metrics_df[f'{metric}_Var'] = var\n",
    "        metrics_df[f'{metric}_Std'] = std\n",
    "        metrics_df[f'{metric}_Avg+/-Std'] = f\"{avg:.2f} Â± {std:.2f}\"\n",
    "\n",
    "        # Add new stat column names to the final order\n",
    "        final_column_order.extend([f'{metric}_Avg', f'{metric}_Var', f'{metric}_Std', f'{metric}_Avg+/-Std'])\n",
    "\n",
    "    # Reorder the DataFrame with the new complete column list\n",
    "    metrics_df = metrics_df[final_column_order]\n",
    "\n",
    "    return metrics_df\n",
    "\n",
    "# --- Main Execution ---\n",
    "\n",
    "# 1. Generate the data\n",
    "expert_list = ['resnet50_pretrained_cosine', 'resnet50_pretrained_one', 'resnet50_pretrained_euclidean', \"resnet50_pretrained_inf\",\"resnet50_pretrained_-inf\"]\n",
    "all_expert_5_runs_results = {expert: load_expert_results(expert) for expert in expert_list}\n",
    "final_predictions_data = get_run_wise_majority_vote(all_expert_5_runs_results)\n",
    "\n",
    "# 2. Call the function to plot and get the enhanced metrics DataFrame\n",
    "metrics_dataframe = calculate_plot_and_create_dataframe(final_predictions_data)\n",
    "\n",
    "# 3. Print the resulting DataFrame\n",
    "print(\"\\n--- Metrics DataFrame with Summary Statistics ---\")\n",
    "print(metrics_dataframe.round(2)) # Rounding for cleaner display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "executionInfo": {
     "elapsed": 19,
     "status": "ok",
     "timestamp": 1756957823555,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "H1ypcQXe01jQ",
    "outputId": "50e3a2f3-e11f-4518-fd32-fd2cb6768525"
   },
   "outputs": [],
   "source": [
    "metrics_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1756957926483,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "F3-isgTz0INm"
   },
   "outputs": [],
   "source": [
    "metrics_dataframe.to_excel(\"Majority_voting_without_caution.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1948,
     "status": "ok",
     "timestamp": 1739413317848,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "Mn3h4wD90UJW",
    "outputId": "b75125c3-36fc-405d-e9fc-78db68b8f1cb"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define class mappings\n",
    "class_mapping = {\"normal\": 0, \"abnormal\": 1}\n",
    "\n",
    "# Example results dictionary (replace with your actual data)\n",
    "expert_list = ['resnet50_pretrained_cosine', 'resnet50_pretrained_one', 'resnet50_pretrained_euclidean', \"resnet50_pretrained_inf\",\"resnet50_pretrained_-inf\", \"Mixture of Experts(5_experts)\",'resnet50_pretrained_random']\n",
    "\n",
    "all_expert_5_runs_results = {\n",
    "    \"cosine\": load_expert_results(expert_list[0]),\n",
    "    \"one\": load_expert_results(expert_list[1]),\n",
    "    \"euclidean\": load_expert_results(expert_list[2]),\n",
    "    \"inf\": load_expert_results(expert_list[3]),\n",
    "    \"-inf\": load_expert_results(expert_list[4]),\n",
    "    \"moe\": load_expert_results(expert_list[5]),\n",
    "    \"random\": load_expert_results(expert_list[6])\n",
    "}\n",
    "\n",
    "# Function to extract the \"output\" value from the prediction string\n",
    "def extract_output(prediction):\n",
    "    if isinstance(prediction, str):\n",
    "        return prediction.strip().lower()\n",
    "    elif isinstance(prediction, dict):\n",
    "        return prediction.get(\"output\", \"\").strip().lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to create a DataFrame with metrics for each expert and run\n",
    "def create_metrics_dataframe(all_expert_5_runs_results, class_mapping):\n",
    "    # Initialize a dictionary to store metrics for each expert\n",
    "    expert_metrics = {}\n",
    "\n",
    "    for expert_name, expert_runs in all_expert_5_runs_results.items():\n",
    "        # Initialize a dictionary to store metrics for each run of the current expert\n",
    "        run_metrics = {}\n",
    "\n",
    "        for run_idx, run_data in enumerate(expert_runs):\n",
    "            actual = []\n",
    "            prediction = []\n",
    "            unknown_classes = []\n",
    "\n",
    "            for img, data in run_data.items():\n",
    "                actual_class = data[\"expectation\"].lower()\n",
    "                pred_class = extract_output(data[\"prediction\"])\n",
    "\n",
    "                if pred_class not in class_mapping:\n",
    "                    unknown_classes.append((img, actual_class, pred_class))\n",
    "                    continue\n",
    "                else:\n",
    "                    pred_label = class_mapping[pred_class]\n",
    "                    prediction.append(pred_label)\n",
    "                    actual_label = class_mapping[actual_class]\n",
    "                    actual.append(actual_label)\n",
    "\n",
    "            # Compute metrics\n",
    "            accuracy = accuracy_score(actual, prediction) * 100\n",
    "            precision = precision_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "            recall = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "            f1 = f1_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "            specificity = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"normal\"], zero_division=0) * 100\n",
    "\n",
    "            # Store metrics for the current run\n",
    "            # run_metrics[f\"accuracy_run{run_idx + 1}\"] = accuracy\n",
    "            # run_metrics[f\"precision_run{run_idx + 1}\"] = precision\n",
    "            # run_metrics[f\"recall_run{run_idx + 1}\"] = recall\n",
    "            # run_metrics[f\"f1_run{run_idx + 1}\"] = f1\n",
    "            # run_metrics[f\"specificity_run{run_idx + 1}\"] = specificity\n",
    "\n",
    "            run_metrics[f\"accuracy_run{run_idx + 1}\"] = round(accuracy, 4)\n",
    "            run_metrics[f\"precision_run{run_idx + 1}\"] = round(precision, 4)\n",
    "            run_metrics[f\"recall_run{run_idx + 1}\"] = round(recall, 4)\n",
    "            run_metrics[f\"f1_run{run_idx + 1}\"] = round(f1, 4)\n",
    "            run_metrics[f\"specificity_run{run_idx + 1}\"] = round(specificity, 4)\n",
    "\n",
    "        # Store metrics for the current expert\n",
    "        expert_metrics[expert_name] = run_metrics\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    metrics_df = pd.DataFrame.from_dict(expert_metrics, orient='index')\n",
    "    column_order = ['accuracy_run1', 'accuracy_run2', 'accuracy_run3', 'accuracy_run4', 'accuracy_run5',\n",
    "                'precision_run1', 'precision_run2', 'precision_run3', 'precision_run4', 'precision_run5',\n",
    "                'recall_run1', 'recall_run2', 'recall_run3', 'recall_run4', 'recall_run5',\n",
    "                'specificity_run1', 'specificity_run2', 'specificity_run3', 'specificity_run4', 'specificity_run5',\n",
    "                'f1_run1', 'f1_run2', 'f1_run3', 'f1_run4', 'f1_run5']\n",
    "    metrics_df = metrics_df.loc[:, column_order]\n",
    "    return metrics_df\n",
    "\n",
    "# Create the DataFrame\n",
    "metrics_df = create_metrics_dataframe(all_expert_5_runs_results, class_mapping)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "K7KDax18HEPq"
   },
   "outputs": [],
   "source": [
    "metrics_df.to_excel(\"Results/MOE_without_caution.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IpD2NqtGPImh"
   },
   "source": [
    "# With caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 10991,
     "status": "ok",
     "timestamp": 1739382089133,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "RKFNIG0oPImi",
    "outputId": "def5683f-c9b7-4815-a13b-fd746542e15b"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define class mappings\n",
    "class_mapping = {\"normal\": 0, \"abnormal\": 1}\n",
    "\n",
    "# Example results dictionary (replace with your actual data)\n",
    "expert_list = ['resnet50_pretrained_cosine_with_caution', 'resnet50_pretrained_one_with_caution', 'resnet50_pretrained_euclidean_with_caution', \"resnet50_pretrained_inf_with_caution\",\"resnet50_pretrained_-inf_with_caution\", \"Mixture of Experts with caution(5_experts)\"]\n",
    "\n",
    "all_expert_5_runs_results = {\n",
    "    \"cosine_with_caution\": load_expert_results(expert_list[0]),\n",
    "    \"one_with_caution\": load_expert_results(expert_list[1]),\n",
    "    \"euclidean_with_caution\": load_expert_results(expert_list[2]),\n",
    "    \"inf_with_caution\": load_expert_results(expert_list[3]),\n",
    "    \"-inf_with_caution\": load_expert_results(expert_list[4]),\n",
    "    \"moe_with_caution\": load_expert_results(expert_list[5])\n",
    "}\n",
    "\n",
    "# Prepare a figure for subplots\n",
    "num_experts = len(all_expert_5_runs_results)\n",
    "num_runs = 5\n",
    "fig, axes = plt.subplots(num_experts, num_runs, figsize=(25, 25))  # 5x5 grid\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# Function to extract the \"output\" value from the prediction string\n",
    "def extract_output(prediction):\n",
    "    # If the prediction is already a string (not JSON), return it directly\n",
    "    if isinstance(prediction, str):\n",
    "        return prediction.strip().lower()\n",
    "    # If it's JSON, parse it and return the \"output\" value\n",
    "    elif isinstance(prediction, dict):\n",
    "        return prediction.get(\"output\", \"\").strip().lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Process results for each expert and each run\n",
    "for expert_idx, (expert_name, expert_runs) in enumerate(all_expert_5_runs_results.items()):\n",
    "    for run_idx, run_data in enumerate(expert_runs):\n",
    "        # Initialize lists for actual and prediction values\n",
    "        actual = []\n",
    "        prediction = []\n",
    "        unknown_classes = []\n",
    "        metrics[f\"{expert_name}_run{run_idx+1}\"] = {}\n",
    "\n",
    "        # Process each image in the run\n",
    "        for img, data in tqdm(run_data.items(), desc=f\"Processing images for {expert_name} run {run_idx+1}\"):\n",
    "            actual_class = data[\"expectation\"].lower()\n",
    "            pred_class = extract_output(data[\"prediction\"])\n",
    "\n",
    "            if pred_class not in class_mapping:\n",
    "                unknown_classes.append((img, actual_class, pred_class))\n",
    "                print(f\"Unknown class in prediction: {pred_class}\")\n",
    "                continue\n",
    "\n",
    "            else:\n",
    "                pred_label = class_mapping[pred_class]\n",
    "                prediction.append(pred_label)\n",
    "                actual_label = class_mapping[actual_class]\n",
    "                actual.append(actual_label)\n",
    "\n",
    "        # Compute metrics\n",
    "        accuracy = accuracy_score(actual, prediction)*100\n",
    "        precision = precision_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)*100\n",
    "        recall = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)*100\n",
    "        f1 = f1_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)*100\n",
    "        specificity = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"normal\"], zero_division=0)*100\n",
    "\n",
    "        metrics[f\"{expert_name}_run{run_idx+1}\"] = {\n",
    "            \"accuracy\": accuracy,\n",
    "            \"precision\": precision,\n",
    "            \"recall\": recall,\n",
    "            \"f1\": f1,\n",
    "            \"specificity\": specificity,\n",
    "            \"unknown_classes\": unknown_classes\n",
    "        }\n",
    "\n",
    "        # Compute confusion matrix\n",
    "        matrix = confusion_matrix(actual, prediction, labels=[0, 1])  # Explicitly pass labels\n",
    "\n",
    "        # Plot confusion matrix in a subplot\n",
    "        ax = axes[expert_idx, run_idx]  # Calculate the position in the 5x5 grid\n",
    "        cm_display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=list(class_mapping.keys()))\n",
    "        cm_display.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "        ax.set_title(\n",
    "            f\"{expert_name} run {run_idx+1}\\n\"\n",
    "            f\"Acc: {accuracy:.2f}, Prec: {precision:.2f}, F1: {f1:.2f}\\n\"\n",
    "            f\"Rec: {recall:.2f}, Specif: {specificity:.2f}\"\n",
    "        )\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "# plt.savefig(\"/content/drive/MyDrive/Binary_classification_using_LLMs/Mixture of Experts/Results/moe_1st_experiment.jpeg\", dpi=300)\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "for run_name, metric_values in metrics.items():\n",
    "    print(f\"\\nMetrics for {run_name}:\")\n",
    "    print(f\"Accuracy: {metric_values['accuracy']:.2f}\")\n",
    "    print(f\"Precision: {metric_values['precision']:.2f}\")\n",
    "    print(f\"Recall: {metric_values['recall']:.2f}\")\n",
    "    print(f\"F1 Score: {metric_values['f1']:.2f}\")\n",
    "    print(f\"Specificity: {metric_values['specificity']:.2f}\")\n",
    "    if metric_values[\"unknown_classes\"]:\n",
    "        print(f\"Unknown classes: {metric_values['unknown_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1010,
     "status": "ok",
     "timestamp": 1739382143267,
     "user": {
      "displayName": "Anwesh Nayak",
      "userId": "10812060260948452341"
     },
     "user_tz": -330
    },
    "id": "nJorKhJTPImj",
    "outputId": "767f00b9-655c-4aca-bae8-6e070f31c2e8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "# Define class mappings\n",
    "class_mapping = {\"normal\": 0, \"abnormal\": 1}\n",
    "\n",
    "# Example results dictionary (replace with your actual data)\n",
    "expert_list = ['resnet50_pretrained_cosine_with_caution', 'resnet50_pretrained_one_with_caution', 'resnet50_pretrained_euclidean_with_caution', \"resnet50_pretrained_inf_with_caution\",\"resnet50_pretrained_-inf_with_caution\", \"Mixture of Experts with caution(5_experts)\"]\n",
    "\n",
    "all_expert_5_runs_results = {\n",
    "    \"cosine_with_caution\": load_expert_results(expert_list[0]),\n",
    "    \"one_with_caution\": load_expert_results(expert_list[1]),\n",
    "    \"euclidean_with_caution\": load_expert_results(expert_list[2]),\n",
    "    \"inf_with_caution\": load_expert_results(expert_list[3]),\n",
    "    \"-inf_with_caution\": load_expert_results(expert_list[4]),\n",
    "    \"moe_with_caution\": load_expert_results(expert_list[5])\n",
    "}\n",
    "\n",
    "# Function to extract the \"output\" value from the prediction string\n",
    "def extract_output(prediction):\n",
    "    if isinstance(prediction, str):\n",
    "        return prediction.strip().lower()\n",
    "    elif isinstance(prediction, dict):\n",
    "        return prediction.get(\"output\", \"\").strip().lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Function to create a DataFrame with metrics for each expert and run\n",
    "def create_metrics_dataframe(all_expert_5_runs_results, class_mapping):\n",
    "    # Initialize a dictionary to store metrics for each expert\n",
    "    expert_metrics = {}\n",
    "\n",
    "    for expert_name, expert_runs in all_expert_5_runs_results.items():\n",
    "        # Initialize a dictionary to store metrics for each run of the current expert\n",
    "        run_metrics = {}\n",
    "\n",
    "        for run_idx, run_data in enumerate(expert_runs):\n",
    "            actual = []\n",
    "            prediction = []\n",
    "            unknown_classes = []\n",
    "\n",
    "            for img, data in run_data.items():\n",
    "                actual_class = data[\"expectation\"].lower()\n",
    "                pred_class = extract_output(data[\"prediction\"])\n",
    "\n",
    "                if pred_class not in class_mapping:\n",
    "                    unknown_classes.append((img, actual_class, pred_class))\n",
    "                    continue\n",
    "                else:\n",
    "                    pred_label = class_mapping[pred_class]\n",
    "                    prediction.append(pred_label)\n",
    "                    actual_label = class_mapping[actual_class]\n",
    "                    actual.append(actual_label)\n",
    "\n",
    "            # Compute metrics\n",
    "            accuracy = accuracy_score(actual, prediction) * 100\n",
    "            precision = precision_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "            recall = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "            f1 = f1_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0) * 100\n",
    "            specificity = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"normal\"], zero_division=0) * 100\n",
    "\n",
    "            # Store metrics for the current run\n",
    "            # run_metrics[f\"accuracy_run{run_idx + 1}\"] = accuracy\n",
    "            # run_metrics[f\"precision_run{run_idx + 1}\"] = precision\n",
    "            # run_metrics[f\"recall_run{run_idx + 1}\"] = recall\n",
    "            # run_metrics[f\"f1_run{run_idx + 1}\"] = f1\n",
    "            # run_metrics[f\"specificity_run{run_idx + 1}\"] = specificity\n",
    "\n",
    "            run_metrics[f\"accuracy_run{run_idx + 1}\"] = round(accuracy, 4)\n",
    "            run_metrics[f\"precision_run{run_idx + 1}\"] = round(precision, 4)\n",
    "            run_metrics[f\"recall_run{run_idx + 1}\"] = round(recall, 4)\n",
    "            run_metrics[f\"f1_run{run_idx + 1}\"] = round(f1, 4)\n",
    "            run_metrics[f\"specificity_run{run_idx + 1}\"] = round(specificity, 4)\n",
    "\n",
    "        # Store metrics for the current expert\n",
    "        expert_metrics[expert_name] = run_metrics\n",
    "\n",
    "    # Convert the dictionary to a DataFrame\n",
    "    metrics_df = pd.DataFrame.from_dict(expert_metrics, orient='index')\n",
    "    column_order = ['accuracy_run1', 'accuracy_run2', 'accuracy_run3', 'accuracy_run4', 'accuracy_run5',\n",
    "                'precision_run1', 'precision_run2', 'precision_run3', 'precision_run4', 'precision_run5',\n",
    "                'recall_run1', 'recall_run2', 'recall_run3', 'recall_run4', 'recall_run5',\n",
    "                'specificity_run1', 'specificity_run2', 'specificity_run3', 'specificity_run4', 'specificity_run5',\n",
    "                'f1_run1', 'f1_run2', 'f1_run3', 'f1_run4', 'f1_run5']\n",
    "    metrics_df = metrics_df.loc[:, column_order]\n",
    "    return metrics_df\n",
    "\n",
    "# Create the DataFrame\n",
    "metrics_df = create_metrics_dataframe(all_expert_5_runs_results, class_mapping)\n",
    "print(metrics_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fHK9xQ2fPImk"
   },
   "outputs": [],
   "source": [
    "metrics_df.to_excel(\"Mixture of Experts/Results/MOE_with_caution.xlsx\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyNpcreJQEdKwchU6QkmcCqv",
   "mount_file_id": "1-d4-0c1m_Y-OZTxoacv27W0SWlBklhzT",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
