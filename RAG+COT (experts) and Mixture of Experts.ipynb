{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UMj9okOi2vh2"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yMHvfb2gur4_"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from configparser import ConfigParser\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import textwrap\n",
    "import os\n",
    "import glob\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "import shutil\n",
    "from openai import OpenAI\n",
    "import os\n",
    "import base64\n",
    "import json\n",
    "from io import BytesIO\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, classification_report,precision_recall_fscore_support\n",
    "import json\n",
    "from scipy.stats import wasserstein_distance\n",
    "from scipy.spatial.distance import euclidean\n",
    "from scipy.linalg import norm\n",
    "from scipy.stats import energy_distance\n",
    "from scipy.spatial.distance import mahalanobis\n",
    "from scipy.spatial.distance import braycurtis\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    ConfusionMatrixDisplay,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    ")\n",
    "\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t-u7tbS7uzNf"
   },
   "outputs": [],
   "source": [
    "train_dir = \"path_to_train_directory\"\"\n",
    "test_dir =  \"path_to_test_directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bn1IDkL4Knbv"
   },
   "outputs": [],
   "source": [
    "def get_data(root_directory):\n",
    "    image_text_pairs = []\n",
    "    # for label in os.listdir(root_directory):\n",
    "    for label in [\"lesion\",\"normal\",\"variation in normal\",\"red lesion\"]:\n",
    "      label_dir = os.path.join(root_directory, label)\n",
    "      if os.path.isdir(label_dir):\n",
    "        # Iterate through each image in the label directory\n",
    "        for image_file in os.listdir(label_dir):\n",
    "            image_path = os.path.join(label_dir, image_file)\n",
    "            # Check if the path is a file (to avoid subdirectories)\n",
    "            if os.path.isfile(image_path):\n",
    "                # Add the image path and label to the list\n",
    "                if label != 'normal':\n",
    "                    image_text_pairs.append((image_path, 'lesion'))\n",
    "                    continue\n",
    "                image_text_pairs.append((image_path, label))\n",
    "    return image_text_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1FgXjUbrKIw7"
   },
   "outputs": [],
   "source": [
    "test_df = pd.DataFrame(get_data(test_dir), columns=['image path','label'])\n",
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CGfUpjBX9_M_"
   },
   "outputs": [],
   "source": [
    "px.histogram(test_df, y=\"label\", color=\"label\", title=\"Classes Distribution\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "n1_x_vXL4ptc"
   },
   "outputs": [],
   "source": [
    "train_df = pd.DataFrame(get_data(train_dir), columns=['image path','label'])\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HaF6bcTY-oEp"
   },
   "outputs": [],
   "source": [
    "test_label_counts = test_df['label'].value_counts()\n",
    "train_label_counts = train_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "t4392GBR-vnq"
   },
   "outputs": [],
   "source": [
    "print(test_label_counts)\n",
    "print(train_label_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FpByc5aTwi96"
   },
   "source": [
    "# Retriever"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I7k8FU6NwNOQ"
   },
   "outputs": [],
   "source": [
    "os.environ[\"KMP_DUPLICATE_LIB_OK\"] = \"TRUE\"\n",
    "\n",
    "def get_image_encoder(image_encoder):\n",
    "    if torch.cuda.is_available():\n",
    "        device = torch.device(\"cuda\")\n",
    "    else:\n",
    "        device = torch.device(\"cpu\")\n",
    "    print(f\"Using device: {device}\")\n",
    "    if image_encoder == \"resnet50_pretrained\":\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        resnet50 = models.resnet50(pretrained=True)\n",
    "        resnet50 = torch.nn.Sequential(*list(resnet50.children())[:-1])  # Remove classification layer\n",
    "        resnet50.eval()\n",
    "        resnet50.to(device)\n",
    "        return resnet50\n",
    "    elif image_encoder == \"resnet50_finetuned\":\n",
    "        resnet50 = models.resnet50(pretrained=False)\n",
    "        for param in resnet50.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        # Replace the final fully connected layer with a new one\n",
    "        num_features = resnet50.fc.in_features  # Get the number of input features for the fc layer\n",
    "        resnet50.fc = nn.Linear(num_features, 2)  # New trainable classifier layer\n",
    "        path = \"encoder_path.pth\"\n",
    "        resnet50.load_state_dict(torch.load(path, map_location=device))\n",
    "        resnet50 = torch.nn.Sequential(*list(resnet50.children())[:-1])  # Remove classification layer\n",
    "        resnet50.eval()\n",
    "        return resnet50\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_number_from_filename(file_path):\n",
    "    # Get the file name from the path\n",
    "    file_name = os.path.basename(file_path)\n",
    "\n",
    "    # Use regular expressions to find all numbers in the file name\n",
    "    match = re.search(r'\\d+', file_name)\n",
    "    return int(match.group())\n",
    "\n",
    "\n",
    "def get_preprocessor():\n",
    "    preprocess = transforms.Compose([\n",
    "        transforms.Resize(256),\n",
    "        transforms.CenterCrop(224),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "    ])\n",
    "    return preprocess\n",
    "\n",
    "def get_wasserstein_distance(a, b):\n",
    "    return np.array([wasserstein_distance(a[i], b) for i in range(len(a))])\n",
    "\n",
    "def get_euclidean_distance(a, b):\n",
    "    return np.array([euclidean(a[i], b) for i in range(len(a))])\n",
    "\n",
    "def get_mahalanobis_distance(a, b, VI=None):\n",
    "    if VI is None:\n",
    "        # Estimate covariance from dataset embeddings\n",
    "        cov = np.cov(a, rowvar=False) + 1e-6 * np.eye(a.shape[1])\n",
    "        VI = np.linalg.inv(cov)\n",
    "    return np.array([mahalanobis(a[i], b, VI) for i in range(len(a))])\n",
    "\n",
    "def get_cosine_similarity(a, b):\n",
    "    return cosine_similarity(a,b)\n",
    "\n",
    "def get_energy_distance(a, b):\n",
    "    return np.array([energy_distance(a[i], b) for i in range(len(a))])\n",
    "\n",
    "def get_braycurtis_distance(a, b):\n",
    "    return np.array([braycurtis(a[i], b) for i in range(len(a))])\n",
    "\n",
    "def get_norm(a,b,ord):\n",
    "    output_list = []\n",
    "    # to traverse rows\n",
    "    for i in range(len(a)):\n",
    "      a_vec = np.array(a[i])\n",
    "      input = []\n",
    "      for j in range(len(a[i])):\n",
    "          input.append(a_vec[j]-b[j])\n",
    "      norm_value = norm(input, ord = ord)\n",
    "      output_list.append(norm_value)\n",
    "    return np.array(output_list)\n",
    "\n",
    "\n",
    "class Retriever:\n",
    "    def __init__(self,dataset,image_encoder,vector_comparision_metric):\n",
    "        # Load the pre-trained ResNet50 model without the top layer (for feature extraction)\n",
    "        self.dataset = dataset\n",
    "        self.image_encoder = image_encoder\n",
    "        self.vector_comparision_metric = vector_comparision_metric\n",
    "        self.model = get_image_encoder(image_encoder)\n",
    "        self.preprocessor = get_preprocessor()\n",
    "        self.lesion_embeddings_df = None\n",
    "        self.lesion_paths = []\n",
    "        self.normal_embeddings_df = None\n",
    "        self.normal_paths = []\n",
    "        self.index_lesion = None\n",
    "        self.index_normal = None\n",
    "        self.top_5_lesion_df = []\n",
    "        self.top_5_normal_df = []\n",
    "        self.normal_embeddings_path = \"normal_embeddings.pkl\"\n",
    "        self.lesion_embeddings_path = \"lesion_embeddings.pkl\"\n",
    "        self.norm_ord_dict = {\n",
    "            \"one\":1,\n",
    "            \"two\":2,\n",
    "            \"inf\":np.inf,\n",
    "            \"-inf\":-np.inf\n",
    "        }\n",
    "        self.dimension = None\n",
    "\n",
    "    def extract_embedding(self,image_path):\n",
    "      img = Image.open(image_path)\n",
    "      img = self.preprocessor(img).unsqueeze(0)  # Add batch dimension\n",
    "      with torch.no_grad():\n",
    "          embedding = self.model(img)\n",
    "      return embedding.squeeze().numpy()\n",
    "\n",
    "    def generate_embeddings(self,label):\n",
    "        embeddings = []\n",
    "        label_df = self.dataset[self.dataset[\"label\"]==label]\n",
    "        label_df.reset_index(inplace=True)\n",
    "        print(\"Generating embdeddibgs of \"+label+\" images....\")\n",
    "        # print(label_df.index)\n",
    "        # print(label_df.shape)\n",
    "        for idx in tqdm(label_df.index):\n",
    "            # print(idx)\n",
    "            img_path = label_df.iloc[idx][\"image path\"]\n",
    "            embedding = self.extract_embedding(img_path)\n",
    "            embeddings.append(np.array(embedding))\n",
    "        label_df[\"embeddings\"] = embeddings\n",
    "        return label_df\n",
    "\n",
    "\n",
    "    def load_ebmbeddings(self):\n",
    "        # Check and load or generate lesion embeddings\n",
    "        if os.path.exists(self.lesion_embeddings_path):\n",
    "            print(\"Lesion embeddings file found. Loading embeddings...\")\n",
    "            self.lesion_embeddings_df = pd.read_pickle(self.lesion_embeddings_path)\n",
    "        else:\n",
    "            print(\"Lesion embeddings file not found. Generating embeddings...\")\n",
    "            self.lesion_embeddings_df = self.generate_embeddings(\"lesion\")\n",
    "            self.lesion_embeddings_df.to_pickle(self.lesion_embeddings_path)\n",
    "\n",
    "        # Check and load or generate normal embeddings\n",
    "        if os.path.exists(self.normal_embeddings_path):\n",
    "            print(\"Normal embeddings file found. Loading embeddings...\")\n",
    "            self.normal_embeddings_df = pd.read_pickle(self.normal_embeddings_path)\n",
    "        else:\n",
    "            print(\"Normal embeddings file not found. Generating embeddings...\")\n",
    "            self.normal_embeddings_df = self.generate_embeddings(\"normal\")\n",
    "            self.normal_embeddings_df.to_pickle(self.normal_embeddings_path)\n",
    "\n",
    "    # Function to perform a similarity search and retrieve top 10 images\n",
    "    def retrieve_top_k_images(self, query_image_path, embeddings_df,k=5):\n",
    "        image_paths = embeddings_df[\"image path\"]\n",
    "\n",
    "        embeddings = np.vstack(embeddings_df[\"embeddings\"].values)\n",
    "\n",
    "        query_embedding = self.extract_embedding(query_image_path)\n",
    "\n",
    "\n",
    "        if self.vector_comparision_metric == \"cosine\":\n",
    "            query_embedding = np.expand_dims(query_embedding, axis=0)\n",
    "            similarities = get_cosine_similarity(query_embedding,embeddings)\n",
    "            top_k_indices = np.argsort(similarities[0])[::-1][:k]  # Sort in ascending order\n",
    "            # Retrieve the corresponding image paths and similarities\n",
    "            top_k_images = [image_paths[i] for i in top_k_indices]\n",
    "            top_k_similarities = [similarities[0][i] for i in top_k_indices]\n",
    "\n",
    "            top_k_dict = {\n",
    "                \"top k images\":top_k_images,\n",
    "                \"top_k_similarities\": top_k_similarities\n",
    "            }\n",
    "            top_k_df=  pd.DataFrame(top_k_dict)\n",
    "            return top_k_df\n",
    "\n",
    "        elif self.vector_comparision_metric in self.norm_ord_dict.keys():\n",
    "            similarities = get_norm(embeddings, query_embedding,self.norm_ord_dict[self.vector_comparision_metric])\n",
    "            top_k_indices = np.argsort(similarities)[:k]  # Sort in ascending order\n",
    "            # Retrieve the corresponding image paths and similarities\n",
    "            top_k_images = [image_paths[i] for i in top_k_indices]\n",
    "            top_k_similarities = [similarities[i] for i in top_k_indices]\n",
    "\n",
    "            top_k_dict = {\n",
    "                \"top k images\":top_k_images,\n",
    "                \"top_k_similarities\": top_k_similarities\n",
    "            }\n",
    "            top_k_df=  pd.DataFrame(top_k_dict)\n",
    "            return top_k_df\n",
    "\n",
    "        elif self.vector_comparision_metric == \"wasserstein\":\n",
    "            similarities = get_wasserstein_distance(embeddings, query_embedding)\n",
    "            top_k_indices = np.argsort(similarities)[:k]  # Sort in ascending order\n",
    "            # Retrieve the corresponding image paths and similarities\n",
    "            top_k_images = [image_paths[i] for i in top_k_indices]\n",
    "            top_k_similarities = [similarities[i] for i in top_k_indices]\n",
    "\n",
    "            top_k_dict = {\n",
    "                \"top k images\":top_k_images,\n",
    "                \"top_k_similarities\": top_k_similarities\n",
    "            }\n",
    "            top_k_df=  pd.DataFrame(top_k_dict)\n",
    "            return top_k_df\n",
    "\n",
    "\n",
    "        elif self.vector_comparision_metric == \"braycurtis\":\n",
    "            similarities = get_braycurtis_distance(embeddings, query_embedding)\n",
    "            top_k_indices = np.argsort(similarities)[:k]  # smaller = more similar\n",
    "            top_k_images = [image_paths[i] for i in top_k_indices]\n",
    "            top_k_similarities = [similarities[i] for i in top_k_indices]\n",
    "\n",
    "            top_k_dict = {\n",
    "                \"top k images\": top_k_images,\n",
    "                \"top_k_similarities\": top_k_similarities\n",
    "            }\n",
    "            top_k_df = pd.DataFrame(top_k_dict)\n",
    "            return top_k_df\n",
    "\n",
    "\n",
    "        elif self.vector_comparision_metric == \"euclidean\":\n",
    "            similarities = get_euclidean_distance(embeddings, query_embedding)\n",
    "            top_k_indices = np.argsort(similarities)[:k]  # Sort in ascending order\n",
    "            # Retrieve the corresponding image paths and similarities\n",
    "            top_k_images = [image_paths[i] for i in top_k_indices]\n",
    "            top_k_similarities = [similarities[i] for i in top_k_indices]\n",
    "\n",
    "            top_k_dict = {\n",
    "                \"top k images\":top_k_images,\n",
    "                \"top_k_similarities\": top_k_similarities\n",
    "            }\n",
    "            top_k_df=  pd.DataFrame(top_k_dict)\n",
    "            return top_k_df\n",
    "\n",
    "        elif self.vector_comparision_metric == \"energy\":\n",
    "            similarities = get_energy_distance(embeddings, query_embedding)\n",
    "            top_k_indices = np.argsort(similarities)[:k]  # Sort in ascending order\n",
    "            # Retrieve the corresponding image paths and similarities\n",
    "            top_k_images = [image_paths[i] for i in top_k_indices]\n",
    "            top_k_similarities = [similarities[i] for i in top_k_indices]\n",
    "\n",
    "            top_k_dict = {\n",
    "                \"top k images\":top_k_images,\n",
    "                \"top_k_similarities\": top_k_similarities\n",
    "            }\n",
    "            top_k_df=  pd.DataFrame(top_k_dict)\n",
    "            return top_k_df\n",
    "\n",
    "        elif self.vector_comparision_metric == \"mahalanobis\":\n",
    "            similarities = get_mahalanobis_distance(embeddings, query_embedding)\n",
    "            top_k_indices = np.argsort(similarities)[:k]  # smaller = more similar\n",
    "            top_k_images = [image_paths[i] for i in top_k_indices]\n",
    "            top_k_similarities = [similarities[i] for i in top_k_indices]\n",
    "\n",
    "            top_k_dict = {\n",
    "                \"top k images\": top_k_images,\n",
    "                \"top_k_similarities\": top_k_similarities\n",
    "            }\n",
    "            top_k_df = pd.DataFrame(top_k_dict)\n",
    "            return top_k_df\n",
    "\n",
    "\n",
    "        elif self.vector_comparision_metric == \"random\":\n",
    "            top_k_indices = np.random.choice(len(embeddings_df), k, replace=False)\n",
    "            top_k_images = [image_paths[i] for i in top_k_indices]\n",
    "            top_k_similarities = ['random' for i in top_k_indices]\n",
    "\n",
    "            top_k_dict = {\n",
    "                \"top k images\":top_k_images,\n",
    "                \"top_k_similarities\": top_k_similarities\n",
    "            }\n",
    "            top_k_df=  pd.DataFrame(top_k_dict)\n",
    "            return top_k_df\n",
    "\n",
    "    # Retrieve top 5 images from Class A and Class B for a given query image\n",
    "    def retrieve_images(self, query_image_path):\n",
    "        self.top_5_lesion_df = self.retrieve_top_k_images(query_image_path, self.lesion_embeddings_df, k=5)\n",
    "        self.top_5_normal_df = self.retrieve_top_k_images(query_image_path, self.normal_embeddings_df, k=5)\n",
    "        return query_image_path, self.top_5_lesion_df, self.top_5_normal_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CfCeJH2w11b7"
   },
   "source": [
    "# Data Processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OStYL-0U2LEl"
   },
   "outputs": [],
   "source": [
    "def to_base64(path):\n",
    "    with open(path, 'rb') as image_file:\n",
    "        image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    return image_base64\n",
    "\n",
    "class Processor:\n",
    "    def __init__(self, lesion_image_list,normal_image_list, test_image_path,normal_cot_dict,lesion_cot_dict,with_caution = False, max_size=200):\n",
    "        if normal_image_list is None:\n",
    "            normal_image_list = []\n",
    "        self.lesion_image_list = lesion_image_list\n",
    "        self.normal_image_list = normal_image_list\n",
    "        self.normal_cot_dict = normal_cot_dict\n",
    "        self.lesion_cot_dict = lesion_cot_dict\n",
    "        self.max_size = max_size\n",
    "        self.test_image = test_image_path\n",
    "        self.with_caution = with_caution\n",
    "        self.messages = None\n",
    "\n",
    "    def get_image_data(self, img_path):\n",
    "        return self.resize_and_convert_to_base64(img_path)\n",
    "    def get_message(self):\n",
    "        self.messages = [\n",
    "        {\"role\": \"user\", \"content\": self.get_content()},\n",
    "        {\"role\": \"user\", \"content\": [\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/jpg;base64,{self.get_image_data(self.test_image)}\"\n",
    "                }\n",
    "            }]\n",
    "          }\n",
    "    ]\n",
    "\n",
    "    def resize_and_convert_to_base64(self, image_path):\n",
    "        size = self.max_size\n",
    "        # Open the image file\n",
    "        with Image.open(image_path) as img:\n",
    "            # Check the size of the image\n",
    "            width, height = img.size\n",
    "\n",
    "            # Resize if either dimension is greater than 800 pixels\n",
    "            if width > size or height > size:\n",
    "                # Determine the new size while maintaining the aspect ratio\n",
    "                if width > height:\n",
    "                    new_width = size\n",
    "                    new_height = int((size / width) * height)\n",
    "                else:\n",
    "                    new_height = size\n",
    "                    new_width = int((size / height) * width)\n",
    "\n",
    "                # Resize the image\n",
    "                img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "            # Convert the image to base64\n",
    "            buffered = BytesIO()\n",
    "            img.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "            return img_base64\n",
    "\n",
    "    def get_content(self):\n",
    "        main_prompt = {\n",
    "            \"type\": \"text\",\n",
    "            \"text\":\"\"\"Here are some images and their respective thought process and reasons to classify them into their output classes.\n",
    "                      Please act as an image classifier while following a similar thought process and classify the last image as either \"abnormal\" or \"normal\".\n",
    "                      Your response should be a JSON object with the following keys:\n",
    "                      - \"thought\"\n",
    "                      - \"output\"\n",
    "                        \"\"\"\n",
    "        }\n",
    "        main_prompt_with_caution ={\n",
    "            \"type\": \"text\",\n",
    "            \"text\":\"\"\"Here are some images and their respective thought process\n",
    "                      and reasons to classify them into their output classes.\n",
    "                      Please act as an image classifier while following a similar\n",
    "                      thought process and classify the last image as either \"abnormal\" or \"normal\".\n",
    "                      Caution on the side of the abnormal rather than the normal.\n",
    "                      Falsely classifying an image as normal isn't okay.\n",
    "                      Your response should be a JSON object with the following keys:\n",
    "                      - \"thought\"\n",
    "                      - \"output\"\n",
    "                        \"\"\"\n",
    "          }\n",
    "\n",
    "        content_list = [main_prompt_with_caution if self.with_caution else main_prompt]\n",
    "\n",
    "        for img_path in self.lesion_image_list:\n",
    "            content_list.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpg;base64,{self.get_image_data(img_path)}\"\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "\n",
    "            content_list.append(\n",
    "                {\"type\": \"text\", \"text\": self.lesion_cot_dict[img_path]}\n",
    "            )\n",
    "            content_list.append(\n",
    "                {\"type\": \"text\", \"text\": \"Output:abnormal\"}\n",
    "            )\n",
    "        for img_path in self.normal_image_list:\n",
    "            content_list.append(\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpg;base64,{self.get_image_data(img_path)}\"\n",
    "                    }\n",
    "                }\n",
    "            )\n",
    "            content_list.append(\n",
    "                {\"type\": \"text\", \"text\": self.normal_cot_dict[img_path]}\n",
    "            )\n",
    "            content_list.append(\n",
    "                {\"type\": \"text\", \"text\": \"Output:normal\"}\n",
    "            )\n",
    "        return content_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OyCQoSDlb0M5"
   },
   "source": [
    "### Augmentation and generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tvPL_v3nQF8z"
   },
   "outputs": [],
   "source": [
    "# load COT dict\n",
    "def load_cot_dict():\n",
    "    with open(f\"path_to_var_normal_cot_dict\", 'r') as file:\n",
    "        var_normal_cot_dict = json.load(file)\n",
    "\n",
    "    with open(\"path_to_red_lesion_cot_dict\") as file:\n",
    "        red_lesion_cot_dict = json.load(file)\n",
    "\n",
    "    with open(f\"path_to_normal_cot_dict\", 'r') as file:\n",
    "        normal_cot_dict = json.load(file)\n",
    "\n",
    "    with open(f\"path_to_lesion_cot_dict\", 'r') as file:\n",
    "        lesion_cot_dict = json.load(file)\n",
    "        lesion_cot_dict.update(var_normal_cot_dict)\n",
    "        lesion_cot_dict.update(red_lesion_cot_dict)\n",
    "    return lesion_cot_dict,normal_cot_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lbhqYcwUtAVN"
   },
   "outputs": [],
   "source": [
    "def save_results(results,expert_name):\n",
    "  file_name = f\"test_{expert_name}.json\"\n",
    "  with open(file_name, 'w') as file:\n",
    "    json.dump(results, file)\n",
    "  print(f\"Results saved to {file_name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jf4QNA9iBN-Z"
   },
   "outputs": [],
   "source": [
    "def extract_output(prediction):\n",
    "    # Remove the markdown code block syntax (```json and ```)\n",
    "    prediction = prediction.strip().replace('```json\\n', '').replace('```', '')\n",
    "    # Parse the JSON string\n",
    "    prediction_dict = json.loads(prediction)\n",
    "    # Return the \"output\" value\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xkVEgz2KqtJi"
   },
   "source": [
    "# Expert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7ds0efLHqoiQ"
   },
   "outputs": [],
   "source": [
    "class Expert(Retriever,Processor):\n",
    "    def __init__(self,image_encoder,vector_comparision_metric,run_num,train_df,test_df,with_caution = False):\n",
    "        self.with_caution = with_caution\n",
    "        self.expert_name = image_encoder+\"_\"+vector_comparision_metric+\"_with_caution\"+\"_Run\"+run_num if self.with_caution else image_encoder+\"_\"+vector_comparision_metric+\"_Run\"+run_num\n",
    "        self.retriever = Retriever(dataset = train_df,image_encoder = image_encoder,vector_comparision_metric = vector_comparision_metric)\n",
    "        self.image_encoder = image_encoder\n",
    "        self.vector_comparision_metric = vector_comparision_metric\n",
    "        self.client = OpenAI(api_key = \"your_api_key\")\n",
    "        self.lesion_cot_dict = {}\n",
    "        self.normal_cot_dict = {}\n",
    "        self.results = {}\n",
    "        self.test_df = test_df\n",
    "\n",
    "    def load_cot_dict():\n",
    "        with open(f\"path_to_var_normal_cot_dict\", 'r') as file:\n",
    "            var_normal_cot_dict = json.load(file)\n",
    "\n",
    "        with open(\"path_to_red_lesion_cot_dict\") as file:\n",
    "            red_lesion_cot_dict = json.load(file)\n",
    "\n",
    "        with open(f\"path_to_normal_cot_dict\", 'r') as file:\n",
    "            normal_cot_dict = json.load(file)\n",
    "\n",
    "        with open(f\"path_to_lesion_cot_dict\", 'r') as file:\n",
    "            lesion_cot_dict = json.load(file)\n",
    "            lesion_cot_dict.update(var_normal_cot_dict)\n",
    "            lesion_cot_dict.update(red_lesion_cot_dict)\n",
    "            return lesion_cot_dict,normal_cot_dict\n",
    "\n",
    "    def load_ebmbeddings(self):\n",
    "        self.retriever.load_ebmbeddings()\n",
    "\n",
    "\n",
    "    def predict(self,lesion_image_list,normal_image_list,test_image_path):\n",
    "        image_processor = Processor(lesion_image_list,\n",
    "                                    normal_image_list,\n",
    "                                    test_image_path,\n",
    "                                    self.normal_cot_dict,\n",
    "                                    self.lesion_cot_dict,\n",
    "                                    with_caution = self.with_caution)\n",
    "        image_processor.get_message()\n",
    "        messages = image_processor.messages\n",
    "\n",
    "        response = self.client.chat.completions.create(\n",
    "            model='gpt-4o-2024-11-20',\n",
    "            messages=messages,\n",
    "            temperature=0.0,\n",
    "            max_tokens=200\n",
    "        )\n",
    "\n",
    "        return response.choices[0].message.content\n",
    "        # return response\n",
    "    def get_top_k_image_list(self,test_image_path):\n",
    "        query_image_path, top_5_lesion_images, top_5_normal_images= self.retriever.retrieve_images(test_image_path)\n",
    "\n",
    "\n",
    "        lesion_img_list = top_5_lesion_images[\"top k images\"].values\n",
    "        lesion_imgs = [os.path.join('lesion',os.path.basename(path)) for path in lesion_img_list if 'variation' not in path and 'red' not in path]\n",
    "        var_imgs = [os.path.join('variation in normal',os.path.basename(path)) for path in lesion_img_list if 'variation' in path]\n",
    "        red_lesion_imgs = [os.path.join('Red lesions',os.path.basename(path)) for path in lesion_img_list if 'red' in path]\n",
    "        lesion_img_list = lesion_imgs + var_imgs+red_lesion_imgs\n",
    "\n",
    "\n",
    "        normal_img_list = top_5_normal_images[\"top k images\"].values\n",
    "        normal_img_list = [os.path.join('normal',os.path.basename(path)) for path in normal_img_list]\n",
    "        return lesion_img_list,normal_img_list,test_image_path\n",
    "\n",
    "    def get_expert_content_list(self):\n",
    "        random.seed(42)\n",
    "        run_images = random.sample(range(len(self.test_df.index)),1)\n",
    "        for idx in run_images:\n",
    "            expectation = self.test_df[\"label\"][idx]\n",
    "            if expectation == \"lesion\":\n",
    "                expectation = \"abnormal\"\n",
    "            test_image_path = self.test_df[\"image path\"][idx]\n",
    "            lesion_img_list,normal_img_list,test_image_path = self.get_top_k_image_list(test_image_path)\n",
    "            test_image_name = test_image_path[94:]\n",
    "            image_processor = Processor(lesion_img_list,\n",
    "                                        normal_img_list,\n",
    "                                        test_image_path,\n",
    "                                        self.normal_cot_dict,\n",
    "                                        self.lesion_cot_dict,\n",
    "                                        with_caution = self.with_caution)\n",
    "        return  image_processor.get_content()\n",
    "\n",
    "    def get_result(self,test=False,test_size=10):\n",
    "        results= {}\n",
    "        random.seed(42)\n",
    "        run_images = self.test_df.index if not test else random.sample(range(len(self.test_df.index)),test_size)\n",
    "        # print(run_images)\n",
    "        for idx in tqdm(run_images[:10]):\n",
    "            expectation = self.test_df[\"label\"][idx]\n",
    "            if expectation == \"lesion\":\n",
    "                expectation = \"abnormal\"\n",
    "            test_image_path = self.test_df[\"image path\"][idx]\n",
    "            query_image_path, top_5_lesion_images, top_5_normal_images= self.retriever.retrieve_images(test_image_path)\n",
    "\n",
    "\n",
    "            lesion_img_list = top_5_lesion_images[\"top k images\"].values\n",
    "            lesion_imgs = [os.path.join('lesion',os.path.basename(path)) for path in lesion_img_list if 'variation' not in path and 'red' not in path]\n",
    "            var_imgs = [os.path.join('variation in normal',os.path.basename(path)) for path in lesion_img_list if 'variation' in path]\n",
    "            red_lesion_imgs = [os.path.join('Red lesions',os.path.basename(path)) for path in lesion_img_list if 'red' in path]\n",
    "            lesion_img_list = lesion_imgs + var_imgs+red_lesion_imgs\n",
    "\n",
    "\n",
    "            normal_img_list = top_5_normal_images[\"top k images\"].values\n",
    "            normal_img_list = [os.path.join('normal',os.path.basename(path)) for path in normal_img_list]\n",
    "\n",
    "            test_image_name = query_image_path[94:]\n",
    "            prediction = self.predict(\n",
    "                                  lesion_img_list,\n",
    "                                  normal_img_list,\n",
    "                                  query_image_path\n",
    "                                )\n",
    "\n",
    "            pred_dict = extract_output(prediction)\n",
    "            print(pred_dict)\n",
    "            results[test_image_name] = {\n",
    "                \"query_image_path\": query_image_path,\n",
    "                \"expectation\": expectation,\n",
    "                \"prediction\": pred_dict[\"output\"],\n",
    "                \"thought\": pred_dict[\"thought\"]\n",
    "            }\n",
    "            # results[test_image_name] = {\n",
    "            #     \"response\": prediction\n",
    "            # }\n",
    "\n",
    "        self.results = results\n",
    "    def save_results(self):\n",
    "        save_results(self.results,self.expert_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FF8jmMF_bYHv"
   },
   "source": [
    "# Expert 7\n",
    "## RAG: Resnet50(pre trained weights) + Braycurtis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dmSYz9cGbYHx"
   },
   "outputs": [],
   "source": [
    " num_runs = 1\n",
    "for run in range(1,num_runs+1):\n",
    "  expert1 = Expert(\n",
    "      image_encoder = 'resnet50_pretrained',\n",
    "      vector_comparision_metric = 'inf',\n",
    "      # run_num = str(run+1),\n",
    "      run_num = str(run),\n",
    "      train_df = train_df,\n",
    "      test_df = test_df,\n",
    "      with_caution = True\n",
    "  )\n",
    "  print(expert1.expert_name)\n",
    "  expert1.load_ebmbeddings()\n",
    "  expert1.load_cot_dict()\n",
    "  expert1.get_result(test=False,test_size = 1)\n",
    "  expert1.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8gEPY_MbYHy"
   },
   "outputs": [],
   "source": [
    "# expert1.load_ebmbeddings()\n",
    "# expert1.load_cot_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9MVORKckbYHz"
   },
   "outputs": [],
   "source": [
    "# expert1.get_result(test=False)\n",
    "# expert1.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DseBKwZ1bYHz"
   },
   "outputs": [],
   "source": [
    "# expert1.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bAACEf9_1fBa"
   },
   "source": [
    "# Expert 1\n",
    "## RAG: Resnet50(pre trained weights) + Cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ljjSEw3f6_hG"
   },
   "outputs": [],
   "source": [
    "num_runs = 5\n",
    "for run in range(1,2):\n",
    "  expert1 = Expert(\n",
    "      image_encoder = 'resnet50_pretrained',\n",
    "      vector_comparision_metric = 'cosine',\n",
    "      # run_num = str(run+1),\n",
    "      run_num = 'cost_analysis',\n",
    "      train_df = train_df,\n",
    "      test_df = test_df\n",
    "  )\n",
    "  print(expert1.expert_name)\n",
    "  expert1.load_ebmbeddings()\n",
    "  expert1.load_cot_dict()\n",
    "  expert1.get_result(test=False,test_size = 1)\n",
    "  expert1.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MHjxUKfL76nj"
   },
   "outputs": [],
   "source": [
    "# expert1.load_ebmbeddings()\n",
    "# expert1.load_cot_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ENyQj1EL8oxL"
   },
   "outputs": [],
   "source": [
    "# expert1.get_result(test=False)\n",
    "# expert1.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EdmW4RTMnjph"
   },
   "outputs": [],
   "source": [
    "# expert1.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hLFKNI4heKid"
   },
   "source": [
    "# Expert 2\n",
    "## RAG: Resnet50(pre trained weights) + One norm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YMIYldKJeKie"
   },
   "outputs": [],
   "source": [
    "expert3 = Expert(\n",
    "    image_encoder = 'resnet50_pretrained',\n",
    "    vector_comparision_metric = 'euclidean',\n",
    "    train_df = train_df,\n",
    "    test_df = test_df,\n",
    "    with_caution = True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WwWvUqHlVTB6"
   },
   "outputs": [],
   "source": [
    "expert3.expert_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r8LAWXSYeKie"
   },
   "outputs": [],
   "source": [
    "expert3.load_ebmbeddings()\n",
    "expert3.load_cot_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ei0CuKzAgoX7"
   },
   "outputs": [],
   "source": [
    "# random.seed(42)\n",
    "# random_test_idx = random.sample(range(len(expert3.test_df.index)),1)[0]\n",
    "# random_test_image = test_df[\"image path\"][random_test_idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9kpYMpieKie"
   },
   "outputs": [],
   "source": [
    "expert3.get_result(test=False,test_size=3)\n",
    "expert3.save_results()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "D37u5roNeKie"
   },
   "outputs": [],
   "source": [
    "# expert3.results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "czM90O9ud3mC"
   },
   "source": [
    "# Expert 2\n",
    "## RAG: Resnet50(pre trained weights) + Eucledean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LxUoCMkRd3mD"
   },
   "outputs": [],
   "source": [
    "expert2 = Expert(\n",
    "    image_encoder = 'resnet50_pretrained',\n",
    "    vector_comparision_metric = 'euclidean',\n",
    "    train_df = train_df,\n",
    "    test_df = test_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bzIXZOvld3mD"
   },
   "outputs": [],
   "source": [
    "expert2.load_ebmbeddings()\n",
    "expert2.load_cot_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xrPo-OTid3mD"
   },
   "outputs": [],
   "source": [
    "expert2.get_result(test=True,test_size=2)\n",
    "expert2.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UDZm_aGeVnD"
   },
   "source": [
    "# Expert 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u-QocL9veWB3"
   },
   "source": [
    "# Expert 5\n",
    "## RAG: Resnet50(pre trained weights) + random\n",
    "## normal cot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3EGFYI9VWIk"
   },
   "outputs": [],
   "source": [
    "expert5 = Expert(\n",
    "    image_encoder = 'resnet50_pretrained',\n",
    "    vector_comparision_metric = 'random',\n",
    "    train_df = train_df,\n",
    "    test_df = test_df\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fg4wy0YYV0IW"
   },
   "outputs": [],
   "source": [
    "print(expert5.expert_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GMixykA5VWIl"
   },
   "outputs": [],
   "source": [
    "expert5.load_ebmbeddings()\n",
    "expert5.load_cot_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "fikvzGA2VWIl"
   },
   "outputs": [],
   "source": [
    "expert5.get_result(test=False,test_size=2)\n",
    "expert5.save_results()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0tt9K4CSegZi"
   },
   "source": [
    "# Mixture of Experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wRc7TgVselSW"
   },
   "outputs": [],
   "source": [
    "def to_base64(path):\n",
    "    with open(path, 'rb') as image_file:\n",
    "        image_base64 = base64.b64encode(image_file.read()).decode('utf-8')\n",
    "    return image_base64\n",
    "def load_expert_results(expert_name):\n",
    "  run_number = \"1\"\n",
    "  file_name = f\"Results/{expert_name}_Run{run_number}.json\"\n",
    "  with open(file_name, 'r') as file:\n",
    "    results = json.load(file)\n",
    "  print(f\"Results loaded from {file_name}\")\n",
    "  return results\n",
    "\n",
    "expert_dict = {\n",
    "    \"without_caution\":['resnet50_pretrained_cosine','resnet50_pretrained_one','resnet50_pretrained_euclidean',\"resnet50_pretrained_inf\",\"resnet50_pretrained_-inf\"],\n",
    "    \"with_caution\":['resnet50_pretrained_cosine_with_caution','resnet50_pretrained_one_with_caution','resnet50_pretrained_euclidean_with_caution',\"resnet50_pretrained_inf_with_caution\",\"resnet50_pretrained_-inf_with_caution\"]\n",
    "}\n",
    "\n",
    "class MixtureOfExpertsProcessor:\n",
    "    def __init__(self,test_image_path,img_name ,with_caution = False,run_num = \"1\",max_size=200):\n",
    "        self.run_num = run_num\n",
    "        self.with_caution = with_caution\n",
    "        # self.expert_dict = {\n",
    "        #                     \"without_caution\":['resnet50_pretrained_cosine','resnet50_pretrained_one','resnet50_pretrained_euclidean',\"resnet50_pretrained_inf\",\"resnet50_pretrained_-inf\"],\n",
    "        #                     \"with_caution\":['resnet50_pretrained_cosine_with_caution','resnet50_pretrained_one_with_caution','resnet50_pretrained_euclidean_with_caution',\"resnet50_pretrained_inf_with_caution\",\"resnet50_pretrained_-inf_with_caution\"]\n",
    "        #                     }\n",
    "        self.expert_dict = {\n",
    "                    \"without_caution\":['resnet50_pretrained_cosine','resnet50_pretrained_one','resnet50_pretrained_euclidean',\"resnet50_pretrained_inf\",\"resnet50_pretrained_-inf\"],\n",
    "                    # \"with_caution\":['resnet50_pretrained_cosine_with_caution','resnet50_pretrained_one_with_caution','resnet50_pretrained_euclidean_with_caution',\"resnet50_pretrained_inf_with_caution\",\"resnet50_pretrained_-inf_with_caution\"]\n",
    "                    }\n",
    "        self.expert_list     = ['resnet50_pretrained_cosine','resnet50_pretrained_one','resnet50_pretrained_euclidean']\n",
    "        # self.expert1_results = load_expert_results(self.expert_list[0]+\"_Run\"+self.run_num)\n",
    "        # self.expert2_results = load_expert_results(self.expert_list[1]+\"_Run\"+self.run_num)\n",
    "        # self.expert3_results = load_expert_results(self.expert_list[2]+\"_Run\"+self.run_num)\n",
    "        # self.expert4_results = load_expert_results(self.expert_list[3]+\"_Run\"+self.run_num)\n",
    "        # self.expert5_results = load_expert_results(self.expert_list[4]+\"_Run\"+self.run_num)\n",
    "        self.expert1_results = load_expert_results(self.expert_list[0])\n",
    "        self.expert2_results = load_expert_results(self.expert_list[1])\n",
    "        self.expert3_results = load_expert_results(self.expert_list[2])\n",
    "        self.max_size = max_size\n",
    "        self.test_image = test_image_path\n",
    "        self.img_name = img_name\n",
    "        self.messages = None\n",
    "\n",
    "    def get_image_data(self, img_path):\n",
    "        return self.resize_and_convert_to_base64(img_path)\n",
    "\n",
    "    def resize_and_convert_to_base64(self, image_path):\n",
    "        size = self.max_size\n",
    "        # Open the image file\n",
    "        with Image.open(image_path) as img:\n",
    "            # Check the size of the image\n",
    "            width, height = img.size\n",
    "\n",
    "            # Resize if either dimension is greater than 800 pixels\n",
    "            if width > size or height > size:\n",
    "                # Determine the new size while maintaining the aspect ratio\n",
    "                if width > height:\n",
    "                    new_width = size\n",
    "                    new_height = int((size / width) * height)\n",
    "                else:\n",
    "                    new_height = size\n",
    "                    new_width = int((size / height) * width)\n",
    "\n",
    "                # Resize the image\n",
    "                img = img.resize((new_width, new_height), Image.Resampling.LANCZOS)\n",
    "\n",
    "            # Convert the image to base64\n",
    "            buffered = BytesIO()\n",
    "            img.save(buffered, format=\"PNG\")\n",
    "            img_base64 = base64.b64encode(buffered.getvalue()).decode(\"utf-8\")\n",
    "\n",
    "            return img_base64\n",
    "\n",
    "    def get_content(self):\n",
    "        content_list = [\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\":\"\"\"You will be provided with buccal mucosa image and opinions from three indpendent medcial experts.\n",
    "             You should critically analyze the reports against the image provided and generate an unbiased opininon and label the image as either \"abnormal\" or \"normal\".\n",
    "             Your response should be a JSON object with the following keys:\n",
    "             - \"thought\"\n",
    "             - \"output\"\n",
    "                        \"\"\"\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\":\"Thought of Expert1 is as follows: \"+self.expert1_results[self.img_name][\"thought\"]\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\":\"Thought of Expert2 is as follows: \"+self.expert2_results[self.img_name][\"thought\"]\n",
    "        },\n",
    "        {\n",
    "            \"type\": \"text\",\n",
    "            \"text\":\"Thought of Expert3 is as follows: \"+self.expert3_results[self.img_name][\"thought\"]\n",
    "        }\n",
    "        # ,\n",
    "        # {\n",
    "        #     \"type\": \"text\",\n",
    "        #     \"text\":\"Thought of Expert4 is as follows: \"+self.expert4_results[self.img_name][\"thought\"]\n",
    "        # },\n",
    "        # {\n",
    "        #     \"type\": \"text\",\n",
    "        #     \"text\":\"Thought of Expert5 is as follows: \"+self.expert5_results[self.img_name][\"thought\"]\n",
    "        # }\n",
    "                        ]\n",
    "        return content_list\n",
    "    def get_message(self):\n",
    "        self.messages = [\n",
    "            {\"role\": \"user\", \"content\": self.get_content()},\n",
    "            {\"role\": \"user\", \"content\": [\n",
    "                {\n",
    "                    \"type\": \"image_url\",\n",
    "                    \"image_url\": {\n",
    "                        \"url\": f\"data:image/jpg;base64,{self.get_image_data(self.test_image)}\"\n",
    "                    }\n",
    "                }]\n",
    "             }\n",
    "        ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1WpmGO3-1BJ5"
   },
   "outputs": [],
   "source": [
    "client = OpenAI(api_key = \"your_api_key\")\n",
    "def predict(messages):\n",
    "    response = client.chat.completions.create(\n",
    "        model='gpt-4o-2024-11-20',\n",
    "        messages=messages,\n",
    "        temperature=0.0,\n",
    "        max_tokens=200\n",
    "    )\n",
    "    return response.choices[0].message.content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "B8VGaRk4c6H3"
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EZ9g-xypXAIM"
   },
   "source": [
    "### without caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PIxjA8xKY5Bl"
   },
   "outputs": [],
   "source": [
    "moe_results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2hN6dLl4ddEk"
   },
   "outputs": [],
   "source": [
    "expert1_results = load_expert_results('resnet50_pretrained_cosine')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7Vh-SNyydhbj"
   },
   "outputs": [],
   "source": [
    "len(expert1_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "B0D1dV6d02DT"
   },
   "outputs": [],
   "source": [
    "def extract_output(prediction):\n",
    "    # Remove the markdown code block syntax (```json and ```)\n",
    "    prediction = prediction.strip().replace('```json\\n', '').replace('```', '')\n",
    "    # Parse the JSON string\n",
    "    prediction_dict = json.loads(prediction)\n",
    "    # Return the \"output\" value\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "S7VhlZlq1lAF"
   },
   "outputs": [],
   "source": [
    "not_in_format_predictions = {}\n",
    "for img in tqdm(list(expert1_results.keys())[:10]):\n",
    "  img_path = expert1_results[img][\"query_image_path\"]\n",
    "  moe_processor = MixtureOfExpertsProcessor(img_path,img)\n",
    "  moe_processor.get_message()\n",
    "  messages = moe_processor.messages\n",
    "  prediction = predict(messages)\n",
    "  # print(prediction)\n",
    "  try:\n",
    "    pred_dict = extract_output(prediction)\n",
    "    moe_results[img] = {\n",
    "        \"query_image_path\": img_path,\n",
    "        \"expectation\": expert1_results[img][\"expectation\"],\n",
    "        \"prediction\": pred_dict[\"output\"],\n",
    "        \"thought\": pred_dict[\"thought\"]\n",
    "    }\n",
    "  except:\n",
    "    print(\"not in format\")\n",
    "    not_in_format_predictions[img] = {\n",
    "        \"query_image_path\": img_path,\n",
    "        \"expectation\": expert1_results[img][\"expectation\"],\n",
    "        \"prediction\": prediction,\n",
    "        \"thought\": prediction\n",
    "    }\n",
    "  # print(moe_results)\n",
    "  # break\n",
    "if len(not_in_format_predictions)>0:\n",
    "    moe_results['not_in_format_predictions'] = not_in_format_predictions\n",
    "save_results(moe_results,\"Mixture of Experts(3_experts)_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FL9eouXx0Zdj"
   },
   "outputs": [],
   "source": [
    "extract_output(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kb62ydwRzouD"
   },
   "outputs": [],
   "source": [
    "# moe_processor = MixtureOfExpertsProcessor(img_path,img)\n",
    "# x = moe_processor.expert3_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2g33Nizcz8I6"
   },
   "outputs": [],
   "source": [
    "# x.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_HFbW8KqXIf6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vAT7JPDUXIzs"
   },
   "source": [
    "### with caution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4gAiWY5HXIzt"
   },
   "outputs": [],
   "source": [
    "moe_results_with_caution = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X-AlBNMkXIzt"
   },
   "outputs": [],
   "source": [
    "run_num = 5\n",
    "expert1_results = load_expert_results('resnet50_pretrained_cosine_with_caution_Run'+str(run_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qbrmkUrMXIzt"
   },
   "outputs": [],
   "source": [
    "len(expert1_results.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NJZwqQ9SXIzt"
   },
   "outputs": [],
   "source": [
    "def extract_output(prediction):\n",
    "    # Remove the markdown code block syntax (```json and ```)\n",
    "    prediction = prediction.strip().replace('```json\\n', '').replace('```', '')\n",
    "    # Parse the JSON string\n",
    "    prediction_dict = json.loads(prediction)\n",
    "    # Return the \"output\" value\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L-CbMpetXIzt"
   },
   "outputs": [],
   "source": [
    "not_in_format_predictions = {}\n",
    "not_in_format_predictions = {}\n",
    "for img in tqdm(list(expert1_results.keys())):\n",
    "  img_path = expert1_results[img][\"query_image_path\"]\n",
    "  moe_processor = MixtureOfExpertsProcessor(img_path,img,with_caution=True,run_num = str(run_num))\n",
    "  moe_processor.get_message()\n",
    "  messages = moe_processor.messages\n",
    "  prediction = predict(messages)\n",
    "  # print(prediction)\n",
    "  try:\n",
    "    pred_dict = extract_output(prediction)\n",
    "    moe_results_with_caution[img] = {\n",
    "        \"query_image_path\": img_path,\n",
    "        \"expectation\": expert1_results[img][\"expectation\"],\n",
    "        \"prediction\": pred_dict[\"output\"],\n",
    "        \"thought\": pred_dict[\"thought\"]\n",
    "    }\n",
    "  except:\n",
    "    print(\"not in format\")\n",
    "    not_in_format_predictions[img] = {\n",
    "        \"query_image_path\": img_path,\n",
    "        \"expectation\": expert1_results[img][\"expectation\"],\n",
    "        \"prediction\": prediction,\n",
    "        \"thought\": prediction\n",
    "    }\n",
    "if len(not_in_format_predictions)>0:\n",
    "    moe_results_with_caution['not_in_format_predictions'] = not_in_format_predictions\n",
    "save_results(moe_results_with_caution,\"Mixture of Experts with caution(5_experts)_Run\"+str(run_num))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gK6esurNeoDd"
   },
   "outputs": [],
   "source": [
    "moe_processor = MixtureOfExpertsProcessor(img_path,img,with_caution=True)\n",
    "moe_processor.expert_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "t4IUbtFPmoQX"
   },
   "source": [
    "## MOE results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZUJIknKUsTRE"
   },
   "outputs": [],
   "source": [
    "def load_expert_results(expert_name):\n",
    "  file_name = f\"Mixture of Experts/Results/{expert_name}.json\"\n",
    "  with open(file_name, 'r') as file:\n",
    "    results = json.load(file)\n",
    "  # print(f\"Results loaded from {file_name}\")\n",
    "  return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ui5E6eJCYtxb"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define class mappings\n",
    "class_mapping = {\"normal\": 0, \"abnormal\": 1}\n",
    "\n",
    "# Example results dictionary (replace with your actual data)\n",
    "# expert_list = ['resnet50_pretrained_cosine','resnet50_pretrained_euclidean','resnet50_pretrained_random','resnet50_pretrained_one']\n",
    "results = {\n",
    "    # \"expert1_cosine\": load_expert_results(expert_list[0]),\n",
    "    # \"expert2_euclidean\": load_expert_results(expert_list[1]),\n",
    "    # \"expert3_random\": load_expert_results(expert_list[2]),\n",
    "    # \"expert3_one\": load_expert_results(expert_list[3]),\n",
    "    \"MoE (5 experts)\": load_expert_results(\"Mixture of Experts(5_experts)_Run1\"),\n",
    "    \"MoE (3 experts)\": load_expert_results(\"Mixture of Experts(3_experts)\"),\n",
    "    \"Moe Caution (5 experts)\": load_expert_results(\"Mixture of Experts with caution(5_experts)_Run1\")\n",
    "}\n",
    "\n",
    "# Prepare a figure for subplots\n",
    "num_runs = len(results)\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 12))  # 2x2 grid\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# Function to extract the \"output\" value from the prediction string\n",
    "def extract_output(prediction):\n",
    "    # If the prediction is already a string (not JSON), return it directly\n",
    "    if isinstance(prediction, str):\n",
    "        return prediction.strip().lower()\n",
    "    # If it's JSON, parse it and return the \"output\" value\n",
    "    elif isinstance(prediction, dict):\n",
    "        return prediction.get(\"output\", \"\").strip().lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Process results for each run\n",
    "for idx, (run_name, run_data) in enumerate(results.items()):\n",
    "    # Initialize lists for actual and prediction values\n",
    "    actual = []\n",
    "    prediction = []\n",
    "    unknown_classes = []\n",
    "    metrics[run_name] = {}\n",
    "\n",
    "    # Process each image in the run\n",
    "    for img, data in tqdm(run_data.items(), desc=f\"Processing images for {run_name}\"):\n",
    "        actual_class = data[\"expectation\"].lower()\n",
    "        pred_class = extract_output(data[\"prediction\"])\n",
    "\n",
    "        if pred_class not in class_mapping:\n",
    "            unknown_classes.append((img, actual_class, pred_class))\n",
    "            print(f\"Unknown class in prediction: {pred_class}\")\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            pred_label = class_mapping[pred_class]\n",
    "            prediction.append(pred_label)\n",
    "            actual_label = class_mapping[actual_class]\n",
    "            actual.append(actual_label)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(actual, prediction)\n",
    "    precision = precision_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)\n",
    "    recall = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)\n",
    "    f1 = f1_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)\n",
    "\n",
    "    metrics[run_name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"unknown_classes\": unknown_classes\n",
    "    }\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    matrix = confusion_matrix(actual, prediction, labels=[0, 1])  # Explicitly pass labels\n",
    "\n",
    "    # Plot confusion matrix in a subplot\n",
    "    ax = axes[idx // 2, idx % 2]  # Calculate the position in the 2x2 grid\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=list(class_mapping.keys()))\n",
    "    cm_display.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "    ax.set_title(\n",
    "        f\"{run_name}\\n\"\n",
    "        f\"Acc: {accuracy:.2f}, Prec: {precision:.2f}, Rec: {recall:.2f}, F1: {f1:.2f}\"\n",
    "    )\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "for run_name, metric_values in metrics.items():\n",
    "    print(f\"\\nMetrics for {run_name}:\")\n",
    "    print(f\"Accuracy: {metric_values['accuracy']:.2f}\")\n",
    "    print(f\"Precision: {metric_values['precision']:.2f}\")\n",
    "    print(f\"Recall: {metric_values['recall']:.2f}\")\n",
    "    print(f\"F1 Score: {metric_values['f1']:.2f}\")\n",
    "    if metric_values[\"unknown_classes\"]:\n",
    "        print(f\"Unknown classes: {metric_values['unknown_classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d_aVI2qVPyKy"
   },
   "source": [
    "# MOE without caution results 5 experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0asluGwwvI_"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define class mappings\n",
    "class_mapping = {\"normal\": 0, \"abnormal\": 1}\n",
    "\n",
    "# Example results dictionary (replace with your actual data)\n",
    "expert_list = ['resnet50_pretrained_cosine','resnet50_pretrained_euclidean','resnet50_pretrained_random','resnet50_pretrained_one',\"resnet50_pretrained_inf\",\"resnet50_pretrained_-inf\",\"Mixture of Experts(5_experts)\"]\n",
    "\n",
    "results = {\n",
    "    \"random\": load_expert_results(expert_list[2]),\n",
    "    \"cosine\": load_expert_results(expert_list[0]),\n",
    "    \"one\": load_expert_results(expert_list[3]),\n",
    "    \"euclidean\": load_expert_results(expert_list[1]),\n",
    "    \"inf\": load_expert_results(expert_list[4]),\n",
    "    \"-inf\": load_expert_results(expert_list[5]),\n",
    "    \"moe_results\": load_expert_results(expert_list[6])\n",
    "}\n",
    "\n",
    "# Prepare a figure for subplots\n",
    "num_runs = len(results)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 18))  # 3x3 grid\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# Function to extract the \"output\" value from the prediction string\n",
    "def extract_output(prediction):\n",
    "    # If the prediction is already a string (not JSON), return it directly\n",
    "    if isinstance(prediction, str):\n",
    "        return prediction.strip().lower()\n",
    "    # If it's JSON, parse it and return the \"output\" value\n",
    "    elif isinstance(prediction, dict):\n",
    "        return prediction.get(\"output\", \"\").strip().lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Process results for each run\n",
    "for idx, (run_name, run_data) in enumerate(results.items()):\n",
    "    # Initialize lists for actual and prediction values\n",
    "    actual = []\n",
    "    prediction = []\n",
    "    unknown_classes = []\n",
    "    metrics[run_name] = {}\n",
    "\n",
    "    # Process each image in the run\n",
    "    for img, data in tqdm(run_data.items(), desc=f\"Processing images for {run_name}\"):\n",
    "        actual_class = data[\"expectation\"].lower()\n",
    "        pred_class = extract_output(data[\"prediction\"])\n",
    "\n",
    "        if pred_class not in class_mapping:\n",
    "            unknown_classes.append((img, actual_class, pred_class))\n",
    "            print(f\"Unknown class in prediction: {pred_class}\")\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            pred_label = class_mapping[pred_class]\n",
    "            prediction.append(pred_label)\n",
    "            actual_label = class_mapping[actual_class]\n",
    "            actual.append(actual_label)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(actual, prediction)\n",
    "    precision = precision_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)\n",
    "    recall = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)\n",
    "    f1 = f1_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)\n",
    "\n",
    "    metrics[run_name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"unknown_classes\": unknown_classes\n",
    "    }\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    matrix = confusion_matrix(actual, prediction, labels=[0, 1])  # Explicitly pass labels\n",
    "\n",
    "    # Plot confusion matrix in a subplot\n",
    "    ax = axes[idx // 3, idx % 3]  # Calculate the position in the 3x3 grid\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=list(class_mapping.keys()))\n",
    "    cm_display.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "    ax.set_title(\n",
    "        f\"{run_name}\\n\"\n",
    "        f\"Acc: {accuracy:.2f}, Prec: {precision:.2f}, Rec: {recall:.2f}, F1: {f1:.2f}\"\n",
    "    )\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "for run_name, metric_values in metrics.items():\n",
    "    print(f\"\\nMetrics for {run_name}:\")\n",
    "    print(f\"Accuracy: {metric_values['accuracy']:.2f}\")\n",
    "    print(f\"Precision: {metric_values['precision']:.2f}\")\n",
    "    print(f\"Recall: {metric_values['recall']:.2f}\")\n",
    "    print(f\"F1 Score: {metric_values['f1']:.2f}\")\n",
    "    if metric_values[\"unknown_classes\"]:\n",
    "        print(f\"Unknown classes: {metric_values['unknown_classes']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KCKnll9pP728"
   },
   "source": [
    "# MOE with caution results 5 experts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tEy7ZN2K2z3-"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Define class mappings\n",
    "class_mapping = {\"normal\": 0, \"abnormal\": 1}\n",
    "\n",
    "# Example results dictionary (replace with your actual data)\n",
    "expert_list = ['resnet50_pretrained_cosine_with_caution','resnet50_pretrained_euclidean_with_caution','resnet50_pretrained_random','resnet50_pretrained_one_with_caution',\"resnet50_pretrained_inf_with_caution\",\"resnet50_pretrained_-inf_with_caution\",\"Mixture of Experts with caution(5_experts)\"]\n",
    "\n",
    "results = {\n",
    "    \"random\": load_expert_results(expert_list[2]),\n",
    "    \"cosine_with_caution\": load_expert_results(expert_list[0]),\n",
    "    \"one_with_caution\": load_expert_results(expert_list[3]),\n",
    "    \"euclidean_with_caution\": load_expert_results(expert_list[1]),\n",
    "    \"inf_with_caution\": load_expert_results(expert_list[4]),\n",
    "    \"-inf_with_caution\": load_expert_results(expert_list[5]),\n",
    "    \"moe_results_with_caution\": load_expert_results(expert_list[6])\n",
    "}\n",
    "\n",
    "# Prepare a figure for subplots\n",
    "num_runs = len(results)\n",
    "fig, axes = plt.subplots(3, 3, figsize=(18, 18))  # 3x3 grid\n",
    "\n",
    "metrics = {}\n",
    "\n",
    "# Function to extract the \"output\" value from the prediction string\n",
    "def extract_output(prediction):\n",
    "    # If the prediction is already a string (not JSON), return it directly\n",
    "    if isinstance(prediction, str):\n",
    "        return prediction.strip().lower()\n",
    "    # If it's JSON, parse it and return the \"output\" value\n",
    "    elif isinstance(prediction, dict):\n",
    "        return prediction.get(\"output\", \"\").strip().lower()\n",
    "    else:\n",
    "        return \"\"\n",
    "\n",
    "# Process results for each run\n",
    "for idx, (run_name, run_data) in enumerate(results.items()):\n",
    "    # Initialize lists for actual and prediction values\n",
    "    actual = []\n",
    "    prediction = []\n",
    "    unknown_classes = []\n",
    "    metrics[run_name] = {}\n",
    "\n",
    "    # Process each image in the run\n",
    "    for img, data in tqdm(run_data.items(), desc=f\"Processing images for {run_name}\"):\n",
    "        actual_class = data[\"expectation\"].lower()\n",
    "        pred_class = extract_output(data[\"prediction\"])\n",
    "\n",
    "        if pred_class not in class_mapping:\n",
    "            unknown_classes.append((img, actual_class, pred_class))\n",
    "            print(f\"Unknown class in prediction: {pred_class}\")\n",
    "            continue\n",
    "\n",
    "        else:\n",
    "            pred_label = class_mapping[pred_class]\n",
    "            prediction.append(pred_label)\n",
    "            actual_label = class_mapping[actual_class]\n",
    "            actual.append(actual_label)\n",
    "\n",
    "    # Compute metrics\n",
    "    accuracy = accuracy_score(actual, prediction)\n",
    "    precision = precision_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)\n",
    "    recall = recall_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)\n",
    "    f1 = f1_score(actual, prediction, average='binary', pos_label=class_mapping[\"abnormal\"], zero_division=0)\n",
    "\n",
    "    metrics[run_name] = {\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"f1\": f1,\n",
    "        \"unknown_classes\": unknown_classes\n",
    "    }\n",
    "\n",
    "    # Compute confusion matrix\n",
    "    matrix = confusion_matrix(actual, prediction, labels=[0, 1])  # Explicitly pass labels\n",
    "\n",
    "    # Plot confusion matrix in a subplot\n",
    "    ax = axes[idx // 3, idx % 3]  # Calculate the position in the 3x3 grid\n",
    "    cm_display = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=list(class_mapping.keys()))\n",
    "    cm_display.plot(cmap=plt.cm.Blues, ax=ax)\n",
    "    ax.set_title(\n",
    "        f\"{run_name}\\n\"\n",
    "        f\"Acc: {accuracy:.2f}, Prec: {precision:.2f}, Rec: {recall:.2f}, F1: {f1:.2f}\"\n",
    "    )\n",
    "\n",
    "# Adjust layout and show the plot\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print metrics\n",
    "for run_name, metric_values in metrics.items():\n",
    "    print(f\"\\nMetrics for {run_name}:\")\n",
    "    print(f\"Accuracy: {metric_values['accuracy']:.2f}\")\n",
    "    print(f\"Precision: {metric_values['precision']:.2f}\")\n",
    "    print(f\"Recall: {metric_values['recall']:.2f}\")\n",
    "    print(f\"F1 Score: {metric_values['f1']:.2f}\")\n",
    "    if metric_values[\"unknown_classes\"]:\n",
    "        print(f\"Unknown classes: {metric_values['unknown_classes']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HR0C-Ds8Opw6"
   },
   "outputs": [],
   "source": [
    "def extract_output(prediction):\n",
    "    # Remove the markdown code block syntax (```json and ```)\n",
    "    prediction = prediction.strip().replace('```json\\n', '').replace('```', '')\n",
    "    # Parse the JSON string\n",
    "    prediction_dict = json.loads(prediction)\n",
    "    # Return the \"output\" value\n",
    "    return prediction_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qwbuQWkkM0uY"
   },
   "outputs": [],
   "source": [
    "# = load_expert_results(\"resnet50_pretrained_cosine_with_caution\")\n",
    "with open(f\"Result.json\", 'r') as file:\n",
    "    cosine_expert = json.load(file)['Run1']\n",
    "resnet50_pretrained_cosine_with_caution = {}\n",
    "not_in_format_predictions = {}\n",
    "for img in tqdm(list(cosine_expert.keys())):\n",
    "  img_path = cosine_expert[img][\"query_image_path\"]\n",
    "  prediction = cosine_expert[img][\"prediction\"]\n",
    "  try:\n",
    "    pred_dict = extract_output(prediction)\n",
    "    # print(prediction)\n",
    "    # print(type(pred_dict))\n",
    "    resnet50_pretrained_cosine_with_caution[img] = {\n",
    "        \"query_image_path\": img_path,\n",
    "        \"expectation\": cosine_expert[img][\"expectation\"],\n",
    "        \"prediction\": pred_dict[\"output\"],\n",
    "        \"thought\": pred_dict[\"thought\"]\n",
    "    }\n",
    "  except Exception as e:\n",
    "    # print(e)\n",
    "    # break\n",
    "    print(\"not in format\")\n",
    "    not_in_format_predictions[img] = {\n",
    "        \"query_image_path\": img_path,\n",
    "        \"expectation\": cosine_expert[img][\"expectation\"],\n",
    "        \"prediction\": prediction,\n",
    "        \"thought\": prediction\n",
    "    }\n",
    "\n",
    "if len(not_in_format_predictions)>0:\n",
    "    resnet50_pretrained_cosine_with_caution['not_in_format_predictions'] = not_in_format_predictions\n",
    "# save_results(moe_results,\"Mixture of Experts(5_experts)\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "bAACEf9_1fBa",
    "u-QocL9veWB3"
   ],
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
