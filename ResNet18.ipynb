{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GxFaVWDISwaK",
    "outputId": "bede0af9-150a-4b02-d8d1-66e7e3c58454"
   },
   "outputs": [],
   "source": [
    "%env CUBLAS_WORKSPACE_CONFIG=:4096:8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qRFrg7oy3Y1W",
    "outputId": "eb4393aa-9eba-4122-a1e7-71fd4aeaa107"
   },
   "outputs": [],
   "source": [
    "# 1. Install required packages (if needed)\n",
    "!pip install torchinfo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5InbErDf38LF"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, random_split, DataLoader\n",
    "from torchvision import models, transforms\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torchinfo import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import random\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "bKJQJzRDP4yr"
   },
   "outputs": [],
   "source": [
    "def set_seed(seed: int):\n",
    "    \"\"\"Seed everything for reproducibility.\"\"\"\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    # enforce deterministic algorithms (may slow things down)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    # torch 2.x: fully deterministic\n",
    "    if hasattr(torch, \"use_deterministic_algorithms\"):\n",
    "        torch.use_deterministic_algorithms(True)\n",
    "# os.environ[\"CUBLAS_WORKSPACE_CONFIG\"] = \":4096:8\"\n",
    "# choose your seed\n",
    "seed_list = [3,5,11,1344,2506]\n",
    "SEED = 1344\n",
    "set_seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "CkRCCB75zmp7"
   },
   "outputs": [],
   "source": [
    "MODEL = 'ResNet'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dQqv_b4U3_vp",
    "outputId": "786595d4-232d-4fa2-ce34-34f61e3d6c5d"
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qt-zH4Yu4AVG"
   },
   "outputs": [],
   "source": [
    "train_dir = \"path_to_train_directory\"\n",
    "test_dir =  \"path_to_test_directory\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kdvAdzNbYvdd"
   },
   "outputs": [],
   "source": [
    "def load_image(path):\n",
    "    img = cv2.imread(path)\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, (224, 224))\n",
    "    img = torch.tensor(img).permute(2, 0, 1).float() / 255.0\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RAg62ylZWzw-"
   },
   "outputs": [],
   "source": [
    "def get_data(root_directory):\n",
    "    \"\"\"\n",
    "    Collects image paths from a fixed list of labels.\n",
    "    Any label that is not 'normal' is considered 'abnormal'.\n",
    "    Returns a list of (image_path, label) tuples.\n",
    "    \"\"\"\n",
    "    image_label_dict = {}\n",
    "    for label in [\"lesion\", \"normal\", \"variation in normal\", \"red lesion\"]:\n",
    "    # for label in [\"lesion\", \"normal\", \"red lesion\"]:\n",
    "        label_dir = os.path.join(root_directory, label)\n",
    "        print('Loading Images from:',label_dir)\n",
    "        if os.path.isdir(label_dir):\n",
    "            for image_file in tqdm(os.listdir(label_dir)):\n",
    "                image_path = os.path.join(label_dir, image_file)\n",
    "                if os.path.isfile(image_path):\n",
    "                    if label != 'normal':\n",
    "                        image_label_dict[image_path] = {\n",
    "                            'img':load_image(image_path),\n",
    "                            'label':1,\n",
    "                            'label_name':'abnormal'\n",
    "                            }\n",
    "                    else:\n",
    "                        image_label_dict[image_path] = {\n",
    "                            'img':load_image(image_path),\n",
    "                            'label':0,\n",
    "                            'label_name':'normal'\n",
    "                            }\n",
    "    return image_label_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4YoSuAahWT7e",
    "outputId": "f5517ac6-8c1b-4f80-d661-135bd639224a"
   },
   "outputs": [],
   "source": [
    "train_samples_all = get_data(train_dir)\n",
    "test_samples  = get_data(test_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "2gvhoiNxHaeR",
    "outputId": "a4f4d51b-fa8c-4080-9f15-fdb3a490e192"
   },
   "outputs": [],
   "source": [
    "len(train_samples_all.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "l7mPXv9FGK2n"
   },
   "outputs": [],
   "source": [
    "def split_data(samples_dict, val_ratio=0.2, seed = SEED):\n",
    "    \"\"\"\n",
    "    Splits the samples_dict into training and validation dictionaries.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "    sample_keys = list(samples_dict.keys())\n",
    "    random.shuffle(sample_keys)\n",
    "\n",
    "    val_size = int(len(sample_keys) * val_ratio)\n",
    "    val_keys = sample_keys[:val_size]\n",
    "    train_keys = sample_keys[val_size:]\n",
    "\n",
    "    train_split = {k: samples_dict[k] for k in train_keys}\n",
    "    val_split = {k: samples_dict[k] for k in val_keys}\n",
    "\n",
    "    return train_split, val_split\n",
    "\n",
    "# Apply the split\n",
    "train_samples, val_samples = split_data(train_samples_all)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oxsMlAAGF4r-",
    "outputId": "6b56d045-402d-4fc9-e2e8-439b97f6be61"
   },
   "outputs": [],
   "source": [
    "print(len(train_samples.keys()),len(val_samples.keys()),len(test_samples.keys()))\n",
    "print('Total samples:',len(train_samples.keys())+len(val_samples.keys())+len(test_samples.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3He1D7KzFjZh"
   },
   "outputs": [],
   "source": [
    "class BatchGenerator:\n",
    "    def __init__(self, image_label_dict, batch_size):\n",
    "        \"\"\"\n",
    "        Wraps a list of pairs into an iterable batch‐generator with length.\n",
    "        \"\"\"\n",
    "        self.image_label_dict = image_label_dict\n",
    "        self.batch_size = batch_size\n",
    "\n",
    "    def __len__(self):\n",
    "        # number of batches (ceil so last partial batch counts)\n",
    "        return math.ceil(len(self.image_label_dict.keys()) / self.batch_size)\n",
    "\n",
    "    def __iter__(self):\n",
    "        batch_img, batch_labels,batch_img_path,batch_label_name = [], [], [], []\n",
    "\n",
    "        for img_path in self.image_label_dict.keys():\n",
    "            batch_img_path.append(img_path)\n",
    "            batch_img.append(self.image_label_dict[img_path]['img'])\n",
    "            batch_labels.append(self.image_label_dict[img_path]['label'])\n",
    "            batch_label_name.append(self.image_label_dict[img_path]['label_name'])\n",
    "\n",
    "            if len(batch_img) == self.batch_size:\n",
    "                yield (\n",
    "                    batch_img_path,\n",
    "                    batch_label_name,\n",
    "                    torch.stack(batch_img),\n",
    "                    torch.tensor(batch_labels, dtype=torch.float),\n",
    "\n",
    "                )\n",
    "                batch_img, batch_labels,batch_img_path,batch_label_name = [], [], [], []\n",
    "\n",
    "        # last partial batch\n",
    "        if batch_img:\n",
    "            yield(\n",
    "                    batch_img_path,\n",
    "                    batch_label_name,\n",
    "                    torch.stack(batch_img),\n",
    "                    torch.tensor(batch_labels, dtype=torch.float),\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eBc0-HstqlnJ"
   },
   "outputs": [],
   "source": [
    "train_gen = BatchGenerator(train_samples, batch_size=32)\n",
    "val_gen = BatchGenerator(val_samples, batch_size=32)\n",
    "test_gen = BatchGenerator(test_samples, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "3Xzr9ERfiEGn",
    "outputId": "54a2c5f7-24c3-4c24-e597-2417e5ba94bd"
   },
   "outputs": [],
   "source": [
    "len(train_gen),len(val_gen),len(test_gen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "goOkC-W3JbKi",
    "outputId": "6c44864a-5471-4881-aa1f-1c8caa1c7c64"
   },
   "outputs": [],
   "source": [
    "for pth,label_name,imgs, labels in tqdm(train_gen, desc='Train', leave=False):\n",
    "        imgs, labels = imgs.to(device), labels.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "lRXrLUxcKx6V",
    "outputId": "7246cd9e-41e9-47f1-fdb7-f8225d24c5d1"
   },
   "outputs": [],
   "source": [
    "pth[0],label_name[0],imgs[0].shape,labels[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BHLmwM8PYNdq"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UGGsUm-20LMk"
   },
   "outputs": [],
   "source": [
    "# model = models.resnet18(pretrained=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "R60E9kZa-DTD"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  model = models.resnet18(pretrained=True)\n",
    "  model.fc = nn.Linear(model.fc.in_features, 2)  # Binary classification\n",
    "  model = model.to(device)\n",
    "  for param in model.parameters():\n",
    "    param.requires_grad = False\n",
    "  for param in model.layer4[:].parameters():\n",
    "      param.requires_grad = True\n",
    "  for param in model.fc.parameters():\n",
    "      param.requires_grad = True\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yPHdGuGjYUD0"
   },
   "outputs": [],
   "source": [
    "# help(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YsiVAxUzZ2yy"
   },
   "outputs": [],
   "source": [
    "# for p in model.named_parameters():\n",
    "#     print(p[1].requires_grad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HD--1obuAiIP"
   },
   "outputs": [],
   "source": [
    "# model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "VX3S2Qw8YQtt"
   },
   "outputs": [],
   "source": [
    "# summary(model, input_size=(32, 3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1IYzb9SdJH8P"
   },
   "outputs": [],
   "source": [
    "# --- Training for one epoch -----------------------------------\n",
    "def train_epoch(model, loader, optimizer,criterion, device):\n",
    "    model.train()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    for pth,label_names,images, labels in tqdm(loader, desc='Train', leave=False):\n",
    "        images, labels = images.to(device), labels.to(device).long()\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(images)                  # [batch, 2] raw logits\n",
    "        loss = criterion(outputs, labels)        # CrossEntropyLoss\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc  = total_correct / total_samples\n",
    "    return avg_loss, avg_acc\n",
    "\n",
    "# --- Validation (no threshold sweep) --------------------------\n",
    "def validate_epoch(model, loader,criterion, device):\n",
    "    model.eval()\n",
    "    total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for pth,label_names,images, labels in tqdm(loader, desc='Val', leave=False):\n",
    "            images, labels = images.to(device), labels.to(device).long()\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            total_loss += loss.item() * labels.size(0)\n",
    "            preds = outputs.argmax(dim=1)\n",
    "            total_correct += (preds == labels).sum().item()\n",
    "            total_samples += labels.size(0)\n",
    "\n",
    "    avg_loss = total_loss / total_samples\n",
    "    avg_acc  = total_correct / total_samples\n",
    "    return avg_loss, avg_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4AOFu8iMjVTn",
    "outputId": "5b042609-917e-4050-9b7f-f6acfbb9ca18"
   },
   "outputs": [],
   "source": [
    "total_abnormal_count = 0\n",
    "total_normal_count = 0\n",
    "for key, value in train_samples_all.items():\n",
    "    if value['label'] == 1:\n",
    "        total_abnormal_count += 1\n",
    "    else:\n",
    "        total_normal_count += 1\n",
    "for key, value in test_samples.items():\n",
    "    if value['label'] == 1:\n",
    "        total_abnormal_count += 1\n",
    "    else:\n",
    "        total_normal_count += 1\n",
    "total_samples = len(train_samples_all.keys())+len(test_samples.keys())\n",
    "print(\"Total abnormal images:\", total_abnormal_count)\n",
    "print(\"Total normal images:\", total_normal_count)\n",
    "print('Total samples:',total_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bnwjAC4JiLTH",
    "outputId": "0bd94292-8031-4849-93ec-c08d18f62839"
   },
   "outputs": [],
   "source": [
    "weight = torch.tensor([total_samples/total_normal_count, total_samples/total_abnormal_count]).to(device)\n",
    "# weight = torch.tensor([1.0,3.0]).to(device)\n",
    "weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hFOUk6hg-R9m"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "optimizer = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer= optimizer, mode = 'min', factor=0.1 ,patience=3)\n",
    "# criterion = nn.CrossEntropyLoss(weight = weight)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "# Early stopping config\n",
    "best_val_loss = float('inf')\n",
    "early_stop_counter = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XNWY2pMx-emA",
    "outputId": "052b685b-3611-4aba-b138-089093e57890"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "dir = \"CNN Results/ResNet18/Best Model\"\n",
    "MODEL_NAME = f\"ResNet18_test_{SEED}_loss\"\n",
    "model_path      = os.path.join(dir, f\"{MODEL_NAME}.pth\")\n",
    "best_model_path = os.path.join(dir, f\"{MODEL_NAME}_best.pth\")\n",
    "metrics_path    = os.path.join(dir, f\"{MODEL_NAME}_training_metrics.json\")\n",
    "\n",
    "best_loss = float('inf')\n",
    "patience, epochs_no_improve = 20, 0\n",
    "train_loss_history, train_acc_history = [], []\n",
    "val_loss_history, val_acc_history = [], []\n",
    "lr_history = []\n",
    "num_epochs = 100\n",
    "for epoch in range(1,num_epochs+1):\n",
    "    train_loss, train_acc = train_epoch(model, train_gen, optimizer, criterion, device)\n",
    "    val_loss, val_acc = validate_epoch(model, val_gen, criterion, device)\n",
    "    scheduler.step(val_loss)\n",
    "\n",
    "    # Record metrics\n",
    "    train_loss_history.append(train_loss)\n",
    "    train_acc_history.append(train_acc)\n",
    "    val_loss_history.append(val_loss)\n",
    "    val_acc_history.append(val_acc)\n",
    "    # threshold_history.append(None)  # no threshold logic in current validate\n",
    "    lr_history.append(optimizer.param_groups[0]['lr'])\n",
    "    print(f\"Epoch {epoch:02d}: \"\n",
    "          f\"Train Loss={train_loss:.4f}, Acc={train_acc*100:.2f}% | \"\n",
    "          f\"Val Loss={val_loss:.4f}, Acc={val_acc*100:.2f}% | \"\n",
    "          f\"LR={lr_history[-1]:.6f}\")\n",
    "\n",
    "    # Early stopping and best model save\n",
    "    if val_loss < best_loss:\n",
    "        best_loss = val_loss\n",
    "        torch.save(model.state_dict(), best_model_path)\n",
    "        epochs_no_improve = 0\n",
    "        print(\"  → New best model saved\")\n",
    "    else:\n",
    "        epochs_no_improve += 1\n",
    "        if epochs_no_improve >= patience:\n",
    "            print(f\"Early stopping at epoch {epoch}\")\n",
    "            break\n",
    "\n",
    "# Save final model and metrics\n",
    "torch.save(model.state_dict(), model_path)\n",
    "metrics = {\n",
    "    \"train_loss\": [float(x) for x in train_loss_history],\n",
    "    \"train_accuracy\": [float(x) for x in train_acc_history],\n",
    "    \"val_loss\": [float(x) for x in val_loss_history],\n",
    "    \"val_accuracy\": [float(x) for x in val_acc_history],\n",
    "    \"learning_rate\": [float(x) for x in lr_history]\n",
    "}\n",
    "with open(metrics_path, 'w') as f:\n",
    "    json.dump(metrics, f, indent=4)\n",
    "\n",
    "print(f\"Training complete. Model saved to:\\n  best → {best_model_path}\\n  final → {model_path}\\nMetrics written to {metrics_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 900
    },
    "id": "LOFZqpSesjng",
    "outputId": "30f0f849-9330-4834-893e-5a6b065039a6"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import json\n",
    "\n",
    "# Load metrics\n",
    "with open(metrics_path, 'r') as f:\n",
    "    metrics = json.load(f)\n",
    "\n",
    "train_loss = metrics['train_loss']\n",
    "val_loss = metrics['val_loss']\n",
    "train_acc = metrics['train_accuracy']\n",
    "val_acc = metrics['val_accuracy']\n",
    "lr = metrics['learning_rate']\n",
    "epochs = range(1, len(train_loss) + 1)\n",
    "\n",
    "# Plot Loss\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs, train_loss, label='Train Loss', marker='o')\n",
    "plt.plot(epochs, val_loss, label='Validation Loss', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "# Plot Accuracy\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs, train_acc, label='Train Accuracy', marker='o')\n",
    "plt.plot(epochs, val_acc, label='Validation Accuracy', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Plot Learning Rate\n",
    "plt.figure(figsize=(6, 4))\n",
    "plt.plot(epochs, lr, label='Learning Rate', color='purple', marker='o')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('LR')\n",
    "plt.title('Learning Rate over Epochs')\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6qGU6GWVoyl"
   },
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZqmVplWKVqAB"
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "  model = models.resnet18(pretrained=True)\n",
    "  model.fc = nn.Linear(model.fc.in_features, 2)  # Binary classification\n",
    "  model = model.to(device)\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AEbcyhLsakQv",
    "outputId": "af235d68-90f1-4d24-f768-1493621cf28d"
   },
   "outputs": [],
   "source": [
    "model = get_model()\n",
    "seed = SEED\n",
    "model_path = best_model_path\n",
    "model.load_state_dict(torch.load(model_path, map_location=device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NDjSBv06bOVB",
    "outputId": "54e64d55-4b90-4b2a-c052-a3501a5d8687"
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "model.eval()\n",
    "y_true = []\n",
    "y_pred = []\n",
    "total_loss, total_correct, total_samples = 0.0, 0, 0\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "with torch.no_grad():\n",
    "    for pth,label_names,images, labels in tqdm(test_gen, desc='Test', leave=False):\n",
    "        images, labels = images.to(device), labels.to(device).long()\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        total_loss += loss.item() * labels.size(0)\n",
    "        preds = outputs.argmax(dim=1)\n",
    "        y_true.extend(labels.cpu().numpy())\n",
    "        y_pred.extend(preds.cpu().numpy())\n",
    "        total_correct += (preds == labels).sum().item()\n",
    "        total_samples += labels.size(0)\n",
    "\n",
    "avg_loss = total_loss / total_samples\n",
    "avg_acc  = total_correct / total_samples\n",
    "print(f\"Test Loss={avg_loss:.4f}, Acc={avg_acc*100:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 528
    },
    "id": "mLfHuYlQd6vp",
    "outputId": "9e13441a-9a1a-4e4a-fb0f-85e248fa10e7"
   },
   "outputs": [],
   "source": [
    "# Basic metrics\n",
    "accuracy = accuracy_score(y_true, y_pred)\n",
    "precision = precision_score(y_true, y_pred)\n",
    "recall = recall_score(y_true, y_pred)\n",
    "f1 = f1_score(y_true, y_pred)\n",
    "\n",
    "# --- Confusion matrix ---\n",
    "cm = confusion_matrix(y_true, y_pred)\n",
    "tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "# Specificity = TN / (TN + FP)\n",
    "specificity = tn / (tn + fp) if (tn + fp) != 0 else 0.0\n",
    "\n",
    "print(f\"\\nEvaluation Metrics:\")\n",
    "print(f\"Accuracy : {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Sensitivity : {recall:.4f}\")\n",
    "print(f\"Specificity : {specificity:.4f}\")\n",
    "print(f\"F1 Score : {f1:.4f}\")\n",
    "\n",
    "labels = [\"normal\", \"lesion\"]\n",
    "plt.figure(figsize=(5, 4))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "A100",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
